#!/usr/bin/env python3
"""
Enhanced Early Warning System mit 110+ Features
Kombiniert Dynamic System Features mit Early Warning Signalen

MIGRATION: Migriert zur DataSchema-Architektur f√ºr konsistente Dateninterpretation
"""

import pandas as pd
import numpy as np
import sys
import os
import json
import hashlib
import joblib
from pathlib import Path
from datetime import datetime
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# üöÄ SAUBERE PYTHONPATH-KONFIGURATION
import sys
from pathlib import Path

# F√ºge config/ und bl/ zu Python-Pfad hinzu
project_root = Path(__file__).parent.parent
config_path = project_root / "config"
bl_path = project_root / "bl"

if str(config_path) not in sys.path:
    sys.path.insert(0, str(config_path))
if str(bl_path) not in sys.path:
    sys.path.insert(0, str(bl_path))

# Jetzt kann ProjectPaths importiert werden
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))
from config.paths_config import ProjectPaths

# MIGRATION: DataSchema Architecture imports
from config.data_access_layer import get_data_access
from config.data_schema import get_data_schema

# Globale Konfiguration importieren
from config.global_config import get_global_config, enforce_real_analysis, enforce_real_backtest

# Import f√ºr wissenschaftlich fundierte Schwellwert-Optimierung
from .threshold_optimizer import RobustThresholdOptimizer
from .churn_model_trainer import ChurnModelTrainer
from .churn_evaluator import ChurnEvaluator

class EnhancedEarlyWarningSystem:
    """
    Enhanced Early Warning mit 110+ Features
    
    MIGRATION: Migriert zur DataSchema-Architektur f√ºr konsistente Dateninterpretation
    """
    
    def __init__(self, experiment_id=None):
        self.paths = ProjectPaths()
        
        # MIGRATION: DataSchema Architecture Integration
        self.dal = get_data_access()
        self.schema = get_data_schema()
        
        self.df = None
        self.features_df = None
        self.model = None
        self.data_dictionary = None
        self.feature_names = []  # Initialisiere feature_names
        self.experiment_id = experiment_id  # MINIMALE √ÑNDERUNG: experiment_id hinzuf√ºgen
        
        print("üîí Enhanced Early Warning System initialisiert mit DataSchema-Validierung")
        
    def _get_csv_metadata(self):
        """Ermittelt Metadaten der CSV-Datei f√ºr Modell-Identifikation"""
        csv_path = ProjectPaths.get_input_data_path()
        if os.path.exists(csv_path):
            stat = os.stat(csv_path)
            return {
                'filename': os.path.basename(csv_path),
                'creation_time': stat.st_ctime,
                'size': stat.st_size,
                'modified_time': stat.st_mtime
            }
        return None

    def _find_existing_model_for_csv(self):
        """Findet existierendes Modell f√ºr aktuelle CSV-Datei"""
        csv_metadata = self._get_csv_metadata()
        if not csv_metadata:
            return None
            
        models_dir = ProjectPaths.get_models_directory()
        if not os.path.exists(models_dir):
            return None
            
        # Suche nach Modellen mit passender CSV-Metadaten
        for model_file in os.listdir(models_dir):
            if model_file.endswith('.json') and 'Enhanced_EarlyWarning' in model_file:
                metadata_path = os.path.join(models_dir, model_file)
                try:
                    with open(metadata_path, 'r') as f:
                        metadata = json.load(f)
                    
                    # Pr√ºfe ob CSV-Metadaten √ºbereinstimmen
                    if 'csv_metadata' in metadata:
                        stored_metadata = metadata['csv_metadata']
                        if (stored_metadata.get('filename') == csv_metadata['filename'] and
                            stored_metadata.get('creation_time') == csv_metadata['creation_time'] and
                            stored_metadata.get('size') == csv_metadata['size']):
                            # Modell gefunden - verwende es
                            model_path = metadata_path.replace('.json', '.joblib')
                            if os.path.exists(model_path):
                                return model_path
                except Exception as e:
                    print(f"‚ö†Ô∏è Fehler beim Lesen von {metadata_path}: {e}")
                    continue
        return None

    def _should_save_new_model(self):
        """Pr√ºft ob neues Modell gespeichert werden soll"""
        existing_model = self._find_existing_model_for_csv()
        if existing_model:
            print(f"‚úÖ Existierendes Modell gefunden f√ºr aktuelle CSV: {os.path.basename(existing_model)}")
            print("üìÑ Verwende existierendes Modell - kein Speichern erforderlich")
            return False, existing_model
        else:
            print("üîÑ Kein passendes Modell gefunden - erstelle neues Modell")
            return True, None
    
    def get_csv_metadata(self, csv_path=None):
        """Erstellt Metadaten f√ºr die CSV-Datei"""
        if csv_path is None:
            csv_path = ProjectPaths.get_input_data_path()
        
        try:
            csv_file = Path(csv_path)
            if not csv_file.exists():
                return None
            
            # Datei-Metadaten
            stat = csv_file.stat()
            metadata = {
                'file_path': str(csv_file),
                'file_name': csv_file.name,
                'file_size': stat.st_size,
                'last_modified': stat.st_mtime,
                'last_modified_iso': datetime.fromtimestamp(stat.st_mtime).isoformat()
            }
            
            return metadata
            
        except Exception as e:
            print(f"‚ùå Fehler beim Lesen der CSV-Metadaten: {e}")
            return None
    
    def csv_has_changed(self, stored_csv_metadata):
        """Pr√ºft ob sich die CSV-Datei seit dem letzten Training ge√§ndert hat"""
        if not stored_csv_metadata:
            return True  # Keine Metadaten = neu trainieren
        
        current_metadata = self.get_csv_metadata()
        if not current_metadata:
            return True  # CSV nicht gefunden = neu trainieren
        
        # Vergleiche Datei-Gr√∂√üe und letztes √Ñnderungsdatum
        size_changed = current_metadata['file_size'] != stored_csv_metadata.get('file_size', 0)
        time_changed = current_metadata['last_modified'] != stored_csv_metadata.get('last_modified', 0)
        
        return size_changed or time_changed
    
    def get_latest_model(self):
        """Findet das neueste Enhanced Early Warning Modell"""
        try:
            models_dir = ProjectPaths.get_models_directory()
            if not models_dir.exists():
                return None, None
            
            # Suche nach Enhanced Early Warning Modellen
            model_files = list(models_dir.glob("Enhanced_EarlyWarning_*.joblib"))
            if not model_files:
                return None, None
            
            # Sortiere nach Datum im Dateinamen (neuestes zuerst)
            model_files.sort(key=lambda x: x.stem, reverse=True)
            latest_model = model_files[0]
            
            # Suche zugeh√∂rige Metadaten-Datei
            metadata_file = latest_model.with_suffix('.json')
            if not metadata_file.exists():
                return str(latest_model), None
            
            return str(latest_model), str(metadata_file)
            
        except Exception as e:
            print(f"‚ùå Fehler beim Suchen des neuesten Modells: {e}")
            return None, None
    
    def load_latest_model(self):
        """L√§dt das neueste Modell wenn CSV unver√§ndert ist"""
        try:
            print("üîç Pr√ºfe verf√ºgbare Modelle...")
            
            # Finde neuestes Modell
            model_path, metadata_path = self.get_latest_model()
            if not model_path:
                print("‚ö†Ô∏è Kein trainiertes Modell gefunden")
                return False
            
            # Lade Metadaten
            if not metadata_path:
                print("‚ö†Ô∏è Keine Modell-Metadaten gefunden")
                return False
            
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # Pr√ºfe CSV-√Ñnderungen
            csv_metadata = metadata.get('csv_metadata')
            if self.csv_has_changed(csv_metadata):
                print("üîÑ CSV-Datei hat sich ge√§ndert - Neutraining erforderlich")
                return False
            
            # Lade Modell
            self.model = joblib.load(model_path)
            self.feature_names = metadata.get('feature_names', [])
            self.model_metadata = metadata  # ‚úÖ KRITISCH: Speichere Metadaten f√ºr vereinfachtes Backtest
            
            print(f"‚úÖ Modell geladen: {Path(model_path).name}")
            print(f"üìä Features: {len(self.feature_names)}")
            print(f"üìÖ Erstellt: {metadata.get('created', 'Unknown')}")
            print(f"üéØ CSV unver√§ndert seit: {csv_metadata.get('last_modified_iso', 'Unknown') if csv_metadata else 'Unknown'}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Fehler beim Laden des Modells: {e}")
            return False
    
    def should_retrain_model(self, requested_training_from=None, requested_training_to=None):
        """Bestimmt ob ein Modell neu trainiert werden sollte"""
        try:
            # Pr√ºfe ob es ein Modell gibt
            model_path, metadata_path = self.get_latest_model()
            if not model_path or not metadata_path:
                return True, "Kein trainiertes Modell gefunden"
            
            # Lade Metadaten
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # üîß NEU: Pr√ºfe Training-Zeitr√§ume
            if requested_training_from is not None and requested_training_to is not None:
                stored_training_periods = metadata.get('training_periods', {})
                stored_from = stored_training_periods.get('training_from')
                stored_to = stored_training_periods.get('training_to')
                
                if stored_from != str(requested_training_from) or stored_to != str(requested_training_to):
                    return True, f"Training-Zeitr√§ume haben sich ge√§ndert: {stored_from}-{stored_to} ‚Üí {requested_training_from}-{requested_training_to}"
            
            # Pr√ºfe CSV-√Ñnderungen
            csv_metadata = metadata.get('csv_metadata')
            if self.csv_has_changed(csv_metadata):
                return True, "CSV-Datei hat sich ge√§ndert"
            
            # Pr√ºfe Modell-Alter (optional: > 30 Tage)
            model_created = metadata.get('created')
            if model_created:
                from datetime import datetime, timedelta
                try:
                    created_date = datetime.fromisoformat(model_created.replace('T', ' '))
                    age_days = (datetime.now() - created_date).days
                    if age_days > 30:
                        return True, f"Modell ist {age_days} Tage alt"
                except:
                    pass
            
            return False, "Modell ist aktuell"
            
        except Exception as e:
            return True, f"Fehler beim Pr√ºfen: {e}"
    
    def delete_old_models(self, keep_latest=1):
        """L√∂scht alte Modelle, beh√§lt nur die neuesten"""
        try:
            models_dir = ProjectPaths.get_models_directory()
            if not models_dir.exists():
                return
            
            # Sammle alle Enhanced Early Warning Modelle
            model_files = list(models_dir.glob("Enhanced_EarlyWarning_*.joblib"))
            metadata_files = list(models_dir.glob("Enhanced_EarlyWarning_*.json"))
            
            if len(model_files) <= keep_latest:
                return  # Nichts zu l√∂schen
            
            # Sortiere nach Datum (√§lteste zuerst)
            model_files.sort(key=lambda x: x.stem)
            metadata_files.sort(key=lambda x: x.stem)
            
            # L√∂sche alte Modelle
            models_to_delete = model_files[:-keep_latest]
            metadata_to_delete = metadata_files[:-keep_latest]
            
            deleted_count = 0
            for model_file in models_to_delete:
                try:
                    model_file.unlink()
                    deleted_count += 1
                except:
                    pass
            
            for metadata_file in metadata_to_delete:
                try:
                    metadata_file.unlink()
                except:
                    pass
            
            if deleted_count > 0:
                print(f"üóëÔ∏è {deleted_count} alte Modelle gel√∂scht")
            
        except Exception as e:
            print(f"‚ùå Fehler beim L√∂schen alter Modelle: {e}")
        
    def load_and_enhance_features(self):
        """
        L√§dt und erweitert Features mit Early Warning Signals
        
        MIGRATION: Verwendet DataAccessLayer statt direkter CSV/JSON-Zugriffe
        """
        print("üö® === ENHANCED EARLY WARNING SYSTEM (DataSchema-Migriert) ===")
        
        # MIGRATION: Lade Daten √ºber DataAccessLayer
        print("üîß Lade Daten √ºber DataAccessLayer...")
        try:
            # Verwende bereits geladene Daten falls verf√ºgbar (backward compatibility)
            if hasattr(self, 'data') and self.data is not None:
                print("‚úÖ Verwende bereits geladene Daten aus Stufe 1")
                self.df = self.data
                print(f"‚úÖ Daten aus Stufe 1 geladen: {len(self.df)} Zeilen, {len(self.df.columns)} Spalten")
            else:
                # MIGRATION: Verwende DataAccessLayer f√ºr automatisch validierte Daten
                self.df = self.dal.load_stage0_data()
                print(f"‚úÖ Daten √ºber DataAccessLayer geladen: {len(self.df)} Zeilen, {len(self.df.columns)} Spalten")
                
        except Exception as e:
            print(f"‚ùå Fehler beim Laden √ºber DataAccessLayer: {e}")
            raise ValueError(f"Datenladeung √ºber DataSchema fehlgeschlagen: {e}")
        
        # MIGRATION: Data Dictionary √ºber DataSchema
        print("üîß Lade Data Dictionary √ºber DataSchema...")
        self.data_dictionary = self.schema.data_dictionary
        print(f"‚úÖ Data Dictionary √ºber DataSchema geladen: {len(self.data_dictionary.get('columns', {}))} Features")
        
        # ‚úÖ ZUR√úCK ZUM BEW√ÑHRTEN ORIGINAL (vermeidet doppeltes Feature Engineering)
        print("üìä Erstelle 360+ Dynamic System Features (ORIGINAL)...")
        features_df, base_features = self._create_integrated_features()
        print(f"‚úÖ {len(base_features)} Dynamic System Features")
        
        # F√ºge Early Warning Features hinzu (TEMPOR√ÑR DEAKTIVIERT F√úR PERFORMANCE)
        print("üö® Early Warning Features TEMPOR√ÑR DEAKTIVIERT f√ºr Performance-Test...")
        enhanced_df = features_df  # Verwende nur die Basis-Features
        
        # Dynamische Spaltennamen aus Data Dictionary  
        primary_key_col, timebase_col, target_col = self._get_dynamic_column_names()
        
        # Finale Feature-Liste (WIRD NACH DATA-LEAKAGE-KORREKTUR GESETZT!)
        exclude_cols = [primary_key_col, timebase_col, target_col]
        final_features = [col for col in enhanced_df.columns if col not in exclude_cols]
        
        print(f"üéØ Total Features: {len(final_features)}")
        
        self.features_df = enhanced_df
        # ‚ùå NICHT HIER SETZEN - wird nach Data-Leakage-Korrektur gesetzt!
        # self.feature_names = final_features
        
        return enhanced_df, final_features
    
    def _get_dynamic_column_names(self):
        """Liest Primary Key, Timebase und Target Spaltennamen dynamisch aus Data Dictionary"""
        primary_key_col = 'Kunde'  # Fallback
        timebase_col = 'I_TIMEBASE'  # Fallback
        target_col = self.get_target_column_name()  # Verwende Data Dictionary
        
        if self.data_dictionary and 'columns' in self.data_dictionary:
            for col_name, col_info in self.data_dictionary['columns'].items():
                if col_info.get('role') == 'PRIMARY_KEY':
                    primary_key_col = col_name
                elif col_info.get('role') == 'TIMEBASE':
                    timebase_col = col_name
                elif col_info.get('role') == 'TARGET':
                    target_col = col_name
        
        return primary_key_col, timebase_col, target_col
    
    def _load_step0_output(self):
        """L√§dt Step0 JSON-Ausgabe"""
        try:
            # Suche nach Step0 JSON-Ausgabe
            step0_dir = ProjectPaths.dynamic_system_outputs_directory() / "stage0_cache"
            if step0_dir.exists():
                json_files = list(step0_dir.glob("*.json"))
                if json_files:
                    # Verwende die neueste Datei
                    latest_file = max(json_files, key=lambda x: x.stat().st_mtime)
                    with open(latest_file, 'r', encoding='utf-8') as f:
                        return json.load(f)
            return None
        except Exception as e:
            print(f"‚ö†Ô∏è Fehler beim Laden der Step0 JSON-Ausgabe: {e}")
            return None

    def _calculate_csv_hash(self, file_path: str) -> str:
        """CSV Hash berechnen (wie in Stufe 0)"""
        try:
            # Sample lesen f√ºr Hash-Berechnung (DEPRECATED: Verwende DataAccessLayer)
            try:
                # MIGRATION NOTE: This is only used for hashing, DataAccessLayer handles actual data loading
                df_sample = pd.read_csv(file_path, nrows=1000, sep=';')
            except:
                df_sample = pd.read_csv(file_path, nrows=1000)
                
            # Struktur-Info erstellen (identisch zu Stufe 0)
            structure_info = {
                'columns': list(df_sample.columns),
                'dtypes': {col: str(dtype) for col, dtype in df_sample.dtypes.items()},
                'null_counts': df_sample.isnull().sum().to_dict(),
                'unique_counts': {col: df_sample[col].nunique() for col in df_sample.columns},
                'file_size': os.path.getsize(file_path)
            }
            
            # Hash berechnen
            structure_str = json.dumps(structure_info, sort_keys=True)
            return hashlib.md5(structure_str.encode()).hexdigest()
            
        except Exception as e:
            print(f"‚ùå Fehler beim Hash-Berechnung: {e}")
            return None

    def get_target_column_name(self):
        """Returns the name of the target column as defined in the Data Dictionary, or 'I_Alive' as fallback."""
        if self.data_dictionary and 'columns' in self.data_dictionary:
            for col_name, col_info in self.data_dictionary['columns'].items():
                if col_info.get('role') == 'TARGET':
                    return col_name
        # Verwende Data Dictionary f√ºr Target-Spalte
        if self.data_dictionary and 'columns' in self.data_dictionary:
            for col, info in self.data_dictionary['columns'].items():
                if info.get('role') == 'TARGET':
                    return col
        # Fallback nur wenn Data Dictionary nicht verf√ºgbar
        return 'I_Alive'

    def add_early_warning_features(self, features_df):
        """F√ºgt spezialisierte Early Warning Features hinzu"""
        print("üö® Erstelle Early Warning Features f√ºr Churn-Pr√§diktion...")
        
        # Kopiere Basis-Daten
        enhanced_df = features_df.copy()
        
        # Dynamische Spaltennamen
        primary_key_col, timebase_col, target_col = self._get_dynamic_column_names()
        
        # Gruppiere nach Primary Key f√ºr zeitlich korrekte Aggregationen
        grouped = enhanced_df.groupby(primary_key_col)
        
        # Finde Dynamic Features aus Data Dictionary
        dynamic_features = []
        if self.data_dictionary and 'columns' in self.data_dictionary:
            for col, col_info in self.data_dictionary['columns'].items():
                if col_info.get('role') == 'DYNAMIC_FEATURE' and col in enhanced_df.columns:
                    dynamic_features.append(col)
        else:
            # Fallback: Verwende Data Dictionary-basierte Klassifizierung
            dynamic_features = []
            if self.data_dictionary and 'columns' in self.data_dictionary:
                for col in enhanced_df.columns:
                    if col in self.data_dictionary['columns']:
                        role = self.data_dictionary['columns'][col].get('role', '')
                        if role == 'DYNAMIC_FEATURE' and col not in [primary_key_col, timebase_col, target_col]:
                            dynamic_features.append(col)
        
        print(f"üéØ Erstelle Early Warning Features f√ºr {len(dynamic_features)} Dynamic Features...")
        
        for feature in dynamic_features:
            if feature in enhanced_df.columns:
                try:
                    # üîß KORRIGIERT: Early Warning Features mit optimalen Zeitfenstern
                    # Early Warning Signale f√ºr verschiedene Zeitfenster
                    optimal_windows = self.find_optimal_global_windows(max_windows=5)  # Max 5 optimale Windows
                    ew_windows = optimal_windows  # Verwende optimale Windows
                    for window in ew_windows:
                        # Drastischer R√ºckgang f√ºr jedes Zeitfenster
                        enhanced_df[f'{feature}_ew_major_drop_{window}p'] = grouped[feature].transform(
                            lambda x: ((x.shift(window) - x.shift(window*2)) / (x.shift(window*2) + 1e-8) > 0.3).astype(int)
                        )
                        
                        # Kontinuierlicher R√ºckgang f√ºr jedes Zeitfenster
                        enhanced_df[f'{feature}_ew_decline_{window}p'] = grouped[feature].transform(
                            lambda x: ((x.shift(window) - x.shift(window*1.5)) > 0).astype(int)
                        )
                        
                        # Volatilit√§t f√ºr jedes Zeitfenster
                        enhanced_df[f'{feature}_ew_volatility_{window}p'] = grouped[feature].transform(
                            lambda x: (x.shift(window).rolling(window=window, min_periods=1).std() > x.shift(window).rolling(window=window, min_periods=1).mean() * 0.3).astype(int)
                        )
                    
                    # Kompletter Stopp (1-Monat Lookback - bleibt konstant)
                    enhanced_df[f'{feature}_ew_stop'] = grouped[feature].transform(
                        lambda x: (x.shift(1) == 0).astype(int)
                    )
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è Fehler bei Early Warning Feature {feature}: {e}")
                    continue
        
        # Statische Early Warning Features (NUR f√ºr echte STATIC_FEATURES)
        static_features = []
        for col in enhanced_df.columns:
            if col not in [primary_key_col, timebase_col, target_col]:
                # Pr√ºfe Data Dictionary: Nur STATIC_FEATURES bekommen Early Warning Features
                if col in self.data_dictionary.get("columns", {}):
                    role = self.data_dictionary["columns"][col].get("role", "")
                    if role == "STATIC_FEATURE":
                        static_features.append(col)
        
        print(f"üéØ Erstelle Early Warning Features f√ºr {len(static_features)} Static Features...")
        
        for feature in static_features:
            if feature in enhanced_df.columns:
                try:
                    # Nur f√ºr echte statische Features (nicht f√ºr kategorische)
                    if feature not in self.data_dictionary.get("columns", {}) or \
                       self.data_dictionary["columns"][feature].get("role") == "STATIC_FEATURE":
                        # Digitalisierungsrate Early Warning (nur f√ºr echte STATIC_FEATURES)
                        if 'digital' in feature.lower():
                            enhanced_df[f'{feature}_ew_digital_drop'] = (enhanced_df[feature] < 0.5).astype(int)
                            enhanced_df[f'{feature}_ew_digital_stagnation'] = (enhanced_df[feature] < 0.7).astype(int)
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è Fehler bei Static Early Warning Feature {feature}: {e}")
                    continue
        
        print(f"‚úÖ {len([col for col in enhanced_df.columns if '_ew_' in col])} Early Warning Features erstellt")
        return enhanced_df
    
    def _create_integrated_data_dictionary(self):
        """Erstellt oder l√§dt Data Dictionary f√ºr intelligente Feature-Klassifizierung"""
        dict_path = ProjectPaths.get_data_dictionary_file()
        
        try:
            if os.path.exists(dict_path):
                with open(dict_path, 'r', encoding='utf-8') as f:
                    self.data_dictionary = json.load(f)
                return
            
            # Erstelle neues Data Dictionary
            self.data_dictionary = {"columns": {}}
            columns = self.df.columns.tolist()
            
            # Dynamische Spaltennamen aus Data Dictionary (falls bereits vorhanden)
            primary_key_col, timebase_col, target_col = self._get_dynamic_column_names()
            
            # üß† INTELLIGENTE FEATURE-KLASSIFIZIERUNG
            for col in columns:
                if col == primary_key_col:
                    role = 'PRIMARY_KEY'
                    data_type = 'NUMERIC'
                elif col == timebase_col:
                    role = 'TIMEBASE'
                    data_type = 'NUMERIC'
                elif col == target_col:
                    role = 'TARGET'
                    data_type = 'BINARY'
                else:
                    # üîç INTELLIGENTE FEATURE-ANALYSE
                    role, data_type = self._intelligently_classify_feature(col)
                
                self.data_dictionary["columns"][col] = {
                    "role": role,
                    "data_type": data_type,
                    "auto_classified": True
                }
            
            # Speichere Data Dictionary
            with open(dict_path, 'w', encoding='utf-8') as f:
                json.dump(self.data_dictionary, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            print(f"‚ö†Ô∏è Data Dictionary Fehler: {e}")
            # Minimaler Fallback
            self.data_dictionary = {"columns": {}}
    
    def _intelligently_classify_feature(self, col: str) -> tuple[str, str]:
        """Intelligente Feature-Klassifizierung basierend auf Datenanalyse"""
        
        # Analysiere Datencharakteristiken
        unique_count = self.df[col].nunique()
        null_count = self.df[col].isnull().sum()
        dtype = str(self.df[col].dtype)
        
        # üîç KATEGORISCHE FEATURE-ERKENNUNG
        if dtype == 'object' or unique_count <= 20:
            # Kategorische Features (wie N_DIGITALIZATIONRATE)
            return 'CATEGORICAL_FEATURE', 'CATEGORICAL'
        
        # üîç STATIC vs DYNAMIC FEATURE-ERKENNUNG (basierend auf Datenanalyse)
        elif unique_count <= 50:
            # Features mit wenigen unique values sind typischerweise statisch
            return 'STATIC_FEATURE', 'NUMERIC'
        else:
            # Features mit vielen unique values sind typischerweise dynamisch
            return 'DYNAMIC_FEATURE', 'NUMERIC'
    
    def _create_integrated_features(self):
        """üéØ GENERISCHES Feature-Engineering - adaptiert sich automatisch an verf√ºgbare Daten"""
        print("üéØ Erstelle GENERISCHE Features (adaptiv basierend auf Data-Dictionary)...")
        
        # Kopiere Basis-Daten
        features_df = self.df.copy()
        
        # üîç DETERMINISTISCHE Feature-Erkennung (100% reproduzierbar)
        available_columns = sorted(list(features_df.columns))  # ‚úÖ SORTIERT = DETERMINISTISCH!
        temporal_features = []
        static_features = []
        numeric_features = []
        categorical_features = []
        
        # Dynamische Spaltennamen aus Data Dictionary
        primary_key_col, timebase_col, target_col = self._get_dynamic_column_names()
        
        # üéØ PURE DATA DICTIONARY KLASSIFIZIERUNG (User-gesteuert, keine Fallbacks)
        for col in available_columns:
            if col in [primary_key_col, timebase_col, target_col]:
                continue  # √úberspringe System-Spalten
                
            # ‚úÖ ZWINGEND: Alle Features M√úSSEN im Data Dictionary definiert sein
            if not self.data_dictionary or col not in self.data_dictionary.get("columns", {}):
                raise ValueError(f"‚ùå Feature '{col}' NICHT im Data Dictionary konfiguriert! Bitte in config/data_dictionary.json hinzuf√ºgen.")
                
            # ‚úÖ PURE: Nur Data Dictionary-Rollen respektieren
            dict_role = self.data_dictionary["columns"][col]["role"]
            print(f"   üìö {col}: {dict_role}")
            
            # ‚úÖ EXPLIZITE Rollen-Behandlung (keine automatischen Fallbacks)
            if dict_role == "DYNAMIC_FEATURE":
                temporal_features.append(col)
            elif dict_role == "STATIC_FEATURE":
                static_features.append(col)  # ‚úÖ NUR hier! Keine doppelte Klassifizierung
            elif dict_role == "CATEGORICAL_FEATURE":
                categorical_features.append(col)  # ‚úÖ Kategorische Features
            elif dict_role == "EXCLUDED_FEATURE":
                print(f"   ‚ùå EXCLUDED: {col} (respektiere User-Konfiguration)")
                continue  # ‚úÖ RESPEKTIERE User-Entscheidung
            elif dict_role in ["PRIMARY_KEY", "TIMEBASE", "TARGET"]:
                continue  # System-Spalten bereits √ºbersprungen
            else:
                raise ValueError(f"‚ùå Unbekannte Data Dictionary Rolle: '{dict_role}' f√ºr Feature '{col}'")
        
        # ‚úÖ SORTIERUNG F√úR DETERMINISMUS (kritisch!)
        temporal_features = sorted(temporal_features)
        static_features = sorted(static_features)
        numeric_features = sorted(numeric_features)
        categorical_features = sorted(categorical_features)
        
        print(f"üß† PURE DATA DICTIONARY Feature-Klassifizierung:")
        print(f"   üìä DYNAMIC_FEATURES: {len(temporal_features)} Features")
        print(f"   üìà STATIC_FEATURES: {len(static_features)} Features") 
        print(f"   üè∑Ô∏è CATEGORICAL_FEATURES: {len(categorical_features)} Features")
        print(f"   ‚úÖ Erkannte Dynamic Features: {temporal_features}")
        print(f"   ‚úÖ Erkannte Static Features: {static_features}")
        print(f"   ‚ö†Ô∏è Numeric General Features: 0 (Pure Data Dictionary - keine automatischen Fallbacks)")
        
        # Sortiere f√ºr zeitliche Korrektheit
        features_df = features_df.sort_values([primary_key_col, timebase_col])
        
        print("üîÑ Erstelle ADAPTIVE Feature-Kombinationen...")
        
        # Gruppiere nach Primary Key f√ºr zeitlich korrekte Aggregationen
        grouped = features_df.groupby(primary_key_col)
        feature_list = []
        
        # üöÄ PERFORMANCE-OPTIMIERTES TEMPORAL FEATURE ENGINEERING
        print(f"‚è∞ Verarbeite {len(temporal_features)} temporale Features...")
        
        # Sammle alle neuen Features in Dictionary f√ºr Batch-Concat
        temporal_feature_dict = {}
        
        for feature in temporal_features:
            if feature in features_df.columns:
                try:
                    # üîß KORRIGIERT: Multi-Window Rolling Features mit optimalen Zeitfenstern
                    # üõ°Ô∏è DEBUG: Einfache Data-Leakage-Pr√§vention mit Debug-Ausgaben
                    optimal_windows = self.find_optimal_global_windows(max_windows=5)  # Max 5 optimale Windows
                    print(f"üîç DEBUG: Verwende optimale Windows: {optimal_windows}")
                    windows = optimal_windows  # Optimale Zeitfenster f√ºr Early Warning
                    for window in windows:
                        try:
                            # Rolling Mean f√ºr jedes Zeitfenster (DEBUG: Einfache 0-Werte-Ausschluss)
                            print(f"üîç DEBUG: Verarbeite {feature}_rolling_{window}p_mean...")
                            temporal_feature_dict[f'{feature}_rolling_{window}p_mean'] = grouped[feature].transform(
                                lambda x: x.shift(window).where(x.shift(window) > 0).rolling(window=window, min_periods=1).mean()
                            )
                            print(f"‚úÖ DEBUG: {feature}_rolling_{window}p_mean erfolgreich")
                        except Exception as e:
                            print(f"‚ùå DEBUG: Fehler bei {feature}_rolling_{window}p_mean: {e}")
                            continue
                        
                        try:
                            # Rolling Sum f√ºr jedes Zeitfenster (DEBUG: Einfache 0-Werte-Ausschluss)
                            print(f"üîç DEBUG: Verarbeite {feature}_rolling_{window}p_sum...")
                            temporal_feature_dict[f'{feature}_rolling_{window}p_sum'] = grouped[feature].transform(
                                lambda x: x.shift(window).where(x.shift(window) > 0).rolling(window=window, min_periods=1).sum()
                            )
                            print(f"‚úÖ DEBUG: {feature}_rolling_{window}p_sum erfolgreich")
                        except Exception as e:
                            print(f"‚ùå DEBUG: Fehler bei {feature}_rolling_{window}p_sum: {e}")
                            continue
                        
                        try:
                            # Rolling Std f√ºr jedes Zeitfenster (DEBUG: Einfache 0-Werte-Ausschluss)
                            print(f"üîç DEBUG: Verarbeite {feature}_rolling_{window}p_std...")
                            temporal_feature_dict[f'{feature}_rolling_{window}p_std'] = grouped[feature].transform(
                                lambda x: x.shift(window).where(x.shift(window) > 0).rolling(window=window, min_periods=1).std().fillna(0)
                            )
                            print(f"‚úÖ DEBUG: {feature}_rolling_{window}p_std erfolgreich")
                        except Exception as e:
                            print(f"‚ùå DEBUG: Fehler bei {feature}_rolling_{window}p_std: {e}")
                            continue
                        
                        feature_list.extend([
                            f'{feature}_rolling_{window}p_mean',
                            f'{feature}_rolling_{window}p_sum', 
                            f'{feature}_rolling_{window}p_std'
                        ])
                    
                    # üîß KORRIGIERT: Trend & Change Features mit optimalen Zeitfenstern
                    # üõ°Ô∏è DEBUG: Einfache Data-Leakage-Pr√§vention mit Debug-Ausgaben
                    # Trend Features f√ºr verschiedene Zeitfenster (DEBUG: Einfache 0-Werte-Ausschluss)
                    trend_windows = optimal_windows  # Verwende optimale Windows
                    for window in trend_windows:
                        try:
                            print(f"üîç DEBUG: Verarbeite {feature}_trend_{window}p...")
                            temporal_feature_dict[f'{feature}_trend_{window}p'] = grouped[feature].transform(
                                lambda x: (x.shift(window).where(x.shift(window) > 0) - x.shift(window*2).where(x.shift(window*2) > 0)).fillna(0)
                            )
                            print(f"‚úÖ DEBUG: {feature}_trend_{window}p erfolgreich")
                        except Exception as e:
                            print(f"‚ùå DEBUG: Fehler bei {feature}_trend_{window}p: {e}")
                            continue
                    
                    # Relative Change f√ºr verschiedene Zeitfenster (DEBUG: Einfache 0-Werte-Ausschluss)
                    change_windows = optimal_windows  # Verwende optimale Windows
                    for window in change_windows:
                        try:
                            print(f"üîç DEBUG: Verarbeite {feature}_pct_change_{window}p...")
                            temporal_feature_dict[f'{feature}_pct_change_{window}p'] = grouped[feature].transform(
                                lambda x: x.shift(window).where(x.shift(window) > 0).pct_change(fill_method=None).fillna(0)
                            )
                            print(f"‚úÖ DEBUG: {feature}_pct_change_{window}p erfolgreich")
                        except Exception as e:
                            print(f"‚ùå DEBUG: Fehler bei {feature}_pct_change_{window}p: {e}")
                            continue
                    
                    # Cumulative Features f√ºr verschiedene Zeitfenster (DEBUG: Einfache 0-Werte-Ausschluss)
                    cum_windows = optimal_windows  # Verwende optimale Windows
                    for window in cum_windows:
                        try:
                            print(f"üîç DEBUG: Verarbeite {feature}_cumsum_{window}p...")
                            temporal_feature_dict[f'{feature}_cumsum_{window}p'] = grouped[feature].transform(
                                lambda x: x.shift(window).where(x.shift(window) > 0).cumsum()
                            )
                            print(f"‚úÖ DEBUG: {feature}_cumsum_{window}p erfolgreich")
                        except Exception as e:
                            print(f"‚ùå DEBUG: Fehler bei {feature}_cumsum_{window}p: {e}")
                            continue
                        
                        try:
                            print(f"üîç DEBUG: Verarbeite {feature}_cummax_{window}p...")
                            temporal_feature_dict[f'{feature}_cummax_{window}p'] = grouped[feature].transform(
                                lambda x: x.shift(window).where(x.shift(window) > 0).cummax()
                            )
                            print(f"‚úÖ DEBUG: {feature}_cummax_{window}p erfolgreich")
                        except Exception as e:
                            print(f"‚ùå DEBUG: Fehler bei {feature}_cummax_{window}p: {e}")
                            continue
                    
                    # Activity Patterns f√ºr verschiedene Zeitfenster (DEBUG: Einfache 0-Werte-Ausschluss)
                    activity_windows = optimal_windows  # Verwende optimale Windows
                    for window in activity_windows:
                        try:
                            print(f"üîç DEBUG: Verarbeite {feature}_activity_rate_{window}p...")
                            temporal_feature_dict[f'{feature}_activity_rate_{window}p'] = grouped[feature].transform(
                                lambda x: (x.shift(window).where(x.shift(window) > 0) > 0).rolling(window=window, min_periods=1).mean()
                            )
                            print(f"‚úÖ DEBUG: {feature}_activity_rate_{window}p erfolgreich")
                        except Exception as e:
                            print(f"‚ùå DEBUG: Fehler bei {feature}_activity_rate_{window}p: {e}")
                            continue
                    
                    # F√ºge alle neuen Features zur Liste hinzu
                    for window in [3, 6, 9, 12]:
                        feature_list.extend([
                            f'{feature}_trend_{window}p',
                            f'{feature}_pct_change_{window}p',
                            f'{feature}_cumsum_{window}p',
                            f'{feature}_cummax_{window}p',
                            f'{feature}_activity_rate_{window}p'
                        ])
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è Fehler bei Feature {feature}: {e}")
                    continue
        
        # üöÄ PERFORMANCE-BOOST: Alle temporalen Features auf einmal hinzuf√ºgen  
        if temporal_feature_dict:
            print(f"üöÄ Batch-Concat {len(temporal_feature_dict)} temporale Features...")
            temporal_df = pd.DataFrame(temporal_feature_dict, index=features_df.index)
            features_df = pd.concat([features_df, temporal_df], axis=1)
        
        # üöÄ PERFORMANCE-OPTIMIERTES STATIC FEATURE ENGINEERING  
        if static_features:
            print(f"üìä Verarbeite {len(static_features)} statische Features...")
            
            # Sammle alle statischen Features in Dictionary f√ºr Batch-Concat
            static_feature_dict = {}
            
            for feature in static_features:
                if feature in features_df.columns:
                    try:
                        # Customer-Level Aggregationen
                        static_feature_dict[f'{feature}_customer_mean'] = grouped[feature].transform('mean')
                        static_feature_dict[f'{feature}_customer_max'] = grouped[feature].transform('max')
                        static_feature_dict[f'{feature}_customer_min'] = grouped[feature].transform('min')
                        static_feature_dict[f'{feature}_customer_std'] = grouped[feature].transform('std').fillna(0)
                        
                        feature_list.extend([
                            f'{feature}_customer_mean',
                            f'{feature}_customer_max',
                            f'{feature}_customer_min',
                            f'{feature}_customer_std'
                        ])
                    except Exception as e:
                        print(f"‚ö†Ô∏è Fehler bei Static Feature {feature}: {e}")
                        continue
            
            # üöÄ PERFORMANCE-BOOST: Alle statischen Features auf einmal hinzuf√ºgen
            if static_feature_dict:
                print(f"üöÄ Batch-Concat {len(static_feature_dict)} statische Features...")
                static_df = pd.DataFrame(static_feature_dict, index=features_df.index)
                features_df = pd.concat([features_df, static_df], axis=1)
                
                # Relative Position Features (ben√∂tigen vorherige Features)
                for feature in static_features:
                    if feature in features_df.columns:
                        try:
                            features_df[f'{feature}_vs_mean'] = features_df[feature] - features_df[f'{feature}_customer_mean']
                        except Exception as e:
                            print(f"‚ö†Ô∏è Fehler bei Relative Feature {feature}: {e}")
                            continue
        
        # üöÄ PERFORMANCE-OPTIMIERTES CATEGORICAL FEATURE ENGINEERING
        if categorical_features:
            print(f"üè∑Ô∏è Verarbeite {len(categorical_features)} kategorische Features...")
            
            for feature in categorical_features:
                if feature in features_df.columns:
                    try:
                        feature_series = features_df[feature]
                        
                        # üéØ SPEZIALBEHANDLUNG F√úR N_DIGITALIZATIONRATE
                        if feature == 'N_DIGITALIZATIONRATE':
                            print(f"   üîß OPTIMIERTE N_DIGITALIZATIONRATE Features:")
                            
                            # 1. Kontinuierliches Feature (behalten)
                            features_df[f'{feature}_continuous'] = feature_series.astype(float)
                            feature_list.append(f'{feature}_continuous')
                            
                            # 2. Binary Features f√ºr wichtige Schwellenwerte
                            features_df[f'{feature}_is_digital'] = (feature_series > 0).astype(int)
                            features_df[f'{feature}_is_highly_digital'] = (feature_series >= 0.8).astype(int)
                            features_df[f'{feature}_above_05'] = (feature_series >= 0.5).astype(int)
                            
                            feature_list.extend([
                                f'{feature}_is_digital',
                                f'{feature}_is_highly_digital', 
                                f'{feature}_above_05'
                            ])
                            
                            # 3. Kategorien-Features (ordinal)
                            digital_categories = pd.cut(
                                feature_series, 
                                bins=[-0.1, 0, 0.5, 1.0, 2.1], 
                                labels=['none', 'low', 'medium', 'high']
                            )
                            features_df[f'{feature}_category'] = digital_categories.astype(str)
                            
                            # 4. One-Hot f√ºr Kategorien
                            category_dummies = pd.get_dummies(digital_categories, prefix=f'{feature}_cat')
                            for col in category_dummies.columns:
                                features_df[col] = category_dummies[col]
                                feature_list.append(col)
                            
                            print(f"      ‚úÖ Kontinuierlich + 3 Binary + 4 Kategorien = 8 Features")
                            
                        else:
                            # Standard One-Hot Encoding f√ºr andere kategorische Features
                            dummies = pd.get_dummies(feature_series, prefix=feature, drop_first=True)
                            
                            # F√ºge One-Hot Features hinzu
                            for col in dummies.columns:
                                features_df[col] = dummies[col]
                                feature_list.append(col)
                            
                            print(f"   ‚úÖ {feature}: {len(dummies.columns)} One-Hot Features erstellt")
                            
                    except Exception as e:
                        print(f"‚ö†Ô∏è Fehler bei Categorical Feature {feature}: {e}")
                        continue
        
        # üéØ PURE DATA DICTIONARY: Keine automatischen Numeric Features mehr
        print(f"‚úÖ Pure Data Dictionary System: Alle Features explizit klassifiziert")
        
        # üöÄ PERFORMANCE-OPTIMIERTE CROSS-FEATURE INTERACTIONS
        print("üîó Erstelle Cross-Feature Interactions...")
        
        # Sammle Interaction Features in Dictionary f√ºr Batch-Concat
        interaction_feature_dict = {}
        
        if len(temporal_features) >= 2:
            # DETERMINISTISCHE Top 5 Features (immer sortiert)
            key_features = sorted(temporal_features)[:5]  # ‚úÖ SORTIERT = DETERMINISTISCH!
            for i, feat1 in enumerate(key_features):
                for feat2 in key_features[i+1:]:
                    try:
                        # Ratio (mit Division-by-Zero Protection)
                        interaction_feature_dict[f'ratio_{feat1}_vs_{feat2}'] = (
                            features_df[feat1] / (features_df[feat2] + 0.001)
                        ).fillna(0)
                        feature_list.append(f'ratio_{feat1}_vs_{feat2}')
                    except:
                        continue
        
        # üöÄ PERFORMANCE-BOOST: Alle Interaction Features auf einmal hinzuf√ºgen
        if interaction_feature_dict:
            print(f"üöÄ Batch-Concat {len(interaction_feature_dict)} interaction Features...")
            interaction_df = pd.DataFrame(interaction_feature_dict, index=features_df.index)
            features_df = pd.concat([features_df, interaction_df], axis=1)
        
        # üéØ Behandle problematische Werte (ROBUST)
        print("üõ°Ô∏è Robustheit: Behandle Infinity/NaN-Werte...")
        features_df = features_df.replace([np.inf, -np.inf], np.nan)
        
        # Intelligente NaN-Behandlung
        for col in feature_list:
            if col in features_df.columns:
                if features_df[col].isna().sum() > 0:
                    # F√ºlle mit Median f√ºr numerische, 0 f√ºr andere
                    if features_df[col].dtype in ['int64', 'float64']:
                        fill_value = features_df[col].median()
                        if pd.isna(fill_value):
                            fill_value = 0
                    else:
                        fill_value = 0
                    features_df[col] = features_df[col].fillna(fill_value)
        
        # üîß KORRIGIERT: FINALE Feature-Liste OHNE Data Leakage
        # ‚ùå KEINE Original-Features verwenden (Data Leakage!)
        # ‚úÖ NUR abgeleitete Features verwenden (Rolling, Trend, Early Warning, etc.)
        
        # Sammle NUR abgeleitete Features (keine Original-Features)
        # üõ°Ô∏è ULTIMATIVE DATA LEAKAGE PR√ÑVENTION: ALLE Original-Features ausschlie√üen!
        original_features = temporal_features + static_features  # Das sind die Original-Features
        print(f"üîç DEBUG: Original-Features: {original_features}")
        print(f"üîç DEBUG: Feature-List: {len(feature_list)} Features")
        
        derived_features = [f for f in feature_list if f in features_df.columns and f not in original_features]
        print(f"üîç DEBUG: Abgeleitete Features: {len(derived_features)} Features")
        print(f"üîç DEBUG: Erste 10 abgeleitete Features: {derived_features[:10]}")
        
        # Zus√§tzliche Pr√ºfung: Keine Original-Features in abgeleiteten Features
        leaked_features = [f for f in derived_features if f in original_features]
        if leaked_features:
            print(f"‚ùå DEBUG: DATA LEAKAGE GEFUNDEN: {leaked_features}")
            derived_features = [f for f in derived_features if f not in leaked_features]
            print(f"‚úÖ DEBUG: Data Leakage behoben: {len(derived_features)} Features √ºbrig")
        
        # Sammle One-Hot Features (kategorische Features sind OK)
        one_hot_features = []
        for col in features_df.columns:
            if '_' in col and col not in [primary_key_col, timebase_col, target_col]:
                # One-Hot Features erkennen (enthalten '_' und sind bin√§r)
                if features_df[col].nunique() <= 2:
                    one_hot_features.append(col)
        
        # Kombiniere NUR abgeleitete Features + One-Hot Features
        all_features = derived_features + one_hot_features
        
        # Entferne Duplikate und sortiere
        all_features = sorted(list(set(all_features)))
        
        # üéØ QUALIT√ÑTSKONTROLLE - KONSTANTE FEATURES ENTFERNEN
        valid_features = []
        constant_features_removed = 0
        for feat in all_features:
            if feat in features_df.columns:
                # ‚úÖ TEMPOR√ÑR DEAKTIVIERT: Feature-Entfernung f√ºr Baseline-Vergleich
                # ENTFERNE konstante Features (kein pr√§diktiver Wert)
                if False and features_df[feat].nunique() == 1:  # DEAKTIVIERT
                    print(f"‚ö†Ô∏è Konstantes Feature ENTFERNT: {feat} (Wert: {features_df[feat].iloc[0]})")
                    constant_features_removed += 1
                else:
                    valid_features.append(feat)
        
        print(f"üóëÔ∏è {constant_features_removed} konstante Features entfernt")
        
        print(f"üéØ PURE DATA DICTIONARY Feature-Engineering abgeschlossen:")
        print(f"   üìä Finale Features: {len(valid_features)} (NUR abgeleitete Features)")
        print(f"   ‚è∞ Dynamic Features verarbeitet: {len(temporal_features)}")
        print(f"   üìà Static Features verarbeitet: {len(static_features)}")
        print(f"   ‚úÖ Data Dictionary Compliance: 100% (keine automatischen Fallbacks)")
        print(f"   üöÄ Performance: OPTIMIERT (Batch-Concat eliminiert DataFrame-Fragmentierung)")
        print(f"   üéØ Feature-Qualit√§tskontrolle: BESTANDEN")
        print(f"   ‚ö° Zeitliche Korrektheit: GARANTIERT (nur vergangene Daten)")
        print(f"   üõ°Ô∏è Robustheit: MAXIMAL (Infinity/NaN behandelt)")
        print(f"   üõ°Ô∏è DATA LEAKAGE PR√ÑVENTION: KEINE Original-Features verwendet!")
        
        # ‚úÖ SETZE feature_names NACH Data-Leakage-Korrektur!
        self.feature_names = valid_features
        
        return features_df, valid_features
        
    def create_customer_dataset(self, prediction_timebase=None):
        """Erstellt Customer-Level Dataset f√ºr Training"""
        print(f"\nüìä Customer-Level Dataset (Prediction: {prediction_timebase})")
        
        primary_key_col, timebase_col, target_col = self._get_dynamic_column_names()
        print(f"üéØ Target-Spalte: {target_col}")
        
        # Debug: Zeige verf√ºgbare Zeitr√§ume
        max_timebase = self.features_df[timebase_col].max()
        min_timebase = self.features_df[timebase_col].min()
        print(f"üìÖ Verf√ºgbarer Zeitraum: {min_timebase} - {max_timebase}")
        

        
        customer_records = []
        
        print(f"üîç Debug: {len(self.features_df[primary_key_col].unique())} Kunden gefunden")
        print(f"üîç Debug: Zeitraum {self.features_df[timebase_col].min()} - {self.features_df[timebase_col].max()}")
        print(f"üîç Debug: Prediction Timebase: {prediction_timebase}")
        
        for customer in self.features_df[primary_key_col].unique():
            customer_data = self.features_df[self.features_df[primary_key_col] == customer].copy()
            customer_data = customer_data.sort_values(timebase_col)
            
            # Historische Daten vor Prediction-Periode (Features)
            # Konvertiere prediction_timebase zu int f√ºr korrekten Vergleich
            prediction_timebase_int = int(prediction_timebase) if prediction_timebase else None
            historical = customer_data[customer_data[timebase_col] < prediction_timebase_int]
            
            if len(historical) > 0:
                latest_record = historical.iloc[-1].copy()
                
                # Zuk√ºnftige Daten nach Prediction-Periode (Target)
                future = customer_data[customer_data[timebase_col] >= prediction_timebase_int]
                
                # Churn = wenn I_Alive False ist (Kunde ist nicht mehr aktiv)
                if len(future) > 0:
                    will_churn = (future[target_col] == False).any()
                else:
                    # Keine zuk√ºnftigen Daten: Kunde ist nicht mehr aktiv
                    will_churn = False
                
                # Verwende die originale Target-Spalte, nicht WILL_CHURN
                latest_record[target_col] = int(will_churn)
                customer_records.append(latest_record)
        
        customer_df = pd.DataFrame(customer_records)
        
        # Debug: Zeige Target-Verteilung
        if target_col in customer_df.columns:
            churn_count = (customer_df[target_col] == 1).sum()
            total_count = len(customer_df)
            print(f"‚úÖ {total_count} Kunden, {churn_count} Churns, Churn Rate: {customer_df[target_col].mean():.3f}")
        else:
            print(f"‚ö†Ô∏è Target-Spalte '{target_col}' nicht gefunden!")
            print(f"üîç Verf√ºgbare Spalten: {list(customer_df.columns)}")
        
        return customer_df
        
    def train_enhanced_model(self, prediction_timebase=None, training_from=None, training_to=None):
        """Trainiert Enhanced Early Warning Model mit optimierten Konfigurationen"""
        print(f"\nü§ñ === OPTIMIERTES MODEL TRAINING ===")
        
        # üîß DATA LEAKAGE PR√ÑVENTION: Validiere prediction_timebase
        if prediction_timebase is None:
            raise ValueError("‚ùå DATA LEAKAGE GEF√ÑHRDUNG: prediction_timebase muss gesetzt werden!")
        
        print(f"üõ°Ô∏è DATA LEAKAGE PR√ÑVENTION: prediction_timebase = {prediction_timebase}")
        
        # Features laden falls noch nicht geladen
        if not self.feature_names:
            self.load_and_enhance_features()
        
        
        # Debug: Zeige Target-Verteilung
        target_col = self._get_dynamic_column_names()[2]  # Target-Spalte
        
        print(f"üéØ Target-Verteilung wird sp√§ter angezeigt...")
        
        # Customer Dataset erstellen
        customer_df = self.create_customer_dataset(prediction_timebase=prediction_timebase)
        
        # Debug: Zeige Target-Verteilung
        print(f"üéØ Target-Verteilung:")
        print(f"   {target_col} = 0: {(customer_df[target_col] == 0).sum()}")
        print(f"   {target_col} = 1: {(customer_df[target_col] == 1).sum()}")
        print(f"   Churn Rate: {customer_df[target_col].mean():.3f}")
        
        # Pr√ºfe ob gen√ºgend Churn-Samples vorhanden sind
        churn_samples = (customer_df[target_col] == 1).sum()
        if churn_samples < 10:
            print(f"‚ö†Ô∏è WARNUNG: Nur {churn_samples} Churn-Samples - Modell k√∂nnte Probleme haben")
        
        # Zus√§tzliche Zeitfilterung falls angegeben
        if training_from is not None and training_to is not None:
            print(f"üõ°Ô∏è Zus√§tzliche Zeitfilterung: {training_from} bis {training_to}")
            training_from_int = int(training_from)
            training_to_int = int(training_to)
            timebase_col = self._get_dynamic_column_names()[1]  # Timebase-Spalte
            customer_df = customer_df[
                (customer_df[timebase_col] >= training_from_int) &
                (customer_df[timebase_col] <= training_to_int)
            ]
            print(f"üìä Nach Zeitfilterung: {len(customer_df)} Kunden")
        
        # Features vorbereiten
        X = customer_df[self.feature_names].fillna(0)
        y = customer_df[target_col]
        
        print(f"üìä Training: {len(X)} Samples, {len(self.feature_names)} Features")
        
        # Train/Test Split mit stratify nur wenn beide Klassen vorhanden
        if len(y.unique()) > 1:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.3, random_state=42, stratify=y
            )
        else:
            print("‚ö†Ô∏è WARNUNG: Nur eine Klasse im Target - verwende einfachen Split")
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.3, random_state=42
            )
        
        # ‚úÖ MIGRIERT: Verwende extrahierte ChurnModelTrainer
        print("üîß === MODEL TRAINING MIT EXTRAHIERTER CHURN MODEL TRAINER ===")
        
        # Initialisiere Model Trainer mit Experiment ID
        model_trainer = ChurnModelTrainer(experiment_id=getattr(self, 'experiment_id', None))
        
        # Lade optimierte Konfiguration
        optimized_config = self._load_optimized_config()
        
        # Bestimme Parameter f√ºr Model Trainer
        model_params = None
        if optimized_config:
            print(f"‚úÖ Optimierte Konfiguration gefunden: {optimized_config['model_type']}")
            # Konvertiere zur ChurnModelTrainer Parameter-Format
            model_params = {
                'n_estimators': optimized_config.get('n_estimators', 300),
                'max_depth': optimized_config.get('max_depth', 12),
                'min_samples_split': optimized_config.get('min_samples_split', 5),
                'min_samples_leaf': optimized_config.get('min_samples_leaf', 2),
                'random_state': 42,
                'class_weight': 'balanced',
                'n_jobs': -1
            }
            
            # Sampling anwenden falls konfiguriert
            if optimized_config.get('sampling_config', {}).get('strategy') != 'none':
                X_train, y_train = self._apply_sampling(X_train, y_train, optimized_config['sampling_config'])
                print(f"‚úÖ Sampling angewendet: {optimized_config['sampling_config']['strategy']}")
        else:
            print("‚ö†Ô∏è Keine optimierte Konfiguration - verwende ChurnModelTrainer Defaults")
        
        # Trainiere Model mit ChurnModelTrainer 
        print("üèÉ‚Äç‚ôÇÔ∏è Training mit ChurnModelTrainer...")
        self.model = model_trainer.train_random_forest(
            X_train.values, y_train.values,
            X_test.values, y_test.values,
            params=model_params
        )
        
        # Speichere Model Trainer f√ºr weitere Verwendung
        self.model_trainer = model_trainer
        
        # ‚úÖ MIGRIERT: Evaluation mit extrahierter ChurnEvaluator
        print("üìä === MODEL EVALUATION MIT EXTRAHIERTER CHURN EVALUATOR ===")
        
        # Initialisiere Evaluator
        evaluator = ChurnEvaluator(experiment_id=getattr(self, 'experiment_id', None))
        
        # Evaluation mit ChurnEvaluator
        evaluation_results = evaluator.evaluate_model_performance(
            self.model, 
            X_test.values, y_test.values,
            feature_names=self.feature_names
        )
        
        print(f"üìä ChurnEvaluator Performance:")
        print(f"   üéØ AUC: {evaluation_results.get('auc', 0.0):.3f}")
        print(f"   üéØ Precision: {evaluation_results.get('precision', 0.0):.3f}")
        print(f"   üéØ Recall: {evaluation_results.get('recall', 0.0):.3f}")
        print(f"   üéØ F1-Score: {evaluation_results.get('f1_score', 0.0):.3f}")
        print(f"   üéØ Accuracy: {evaluation_results.get('accuracy', 0.0):.3f}")
        
        # Kompatibilit√§t: Setze legacy Variablen f√ºr Backward Compatibility
        auc_score = evaluation_results.get('auc', 0.0)
        report = {
            'accuracy': evaluation_results.get('accuracy', 0.0),
            '1': {
                'precision': evaluation_results.get('precision', 0.0),
                'recall': evaluation_results.get('recall', 0.0),
                'f1-score': evaluation_results.get('f1_score', 0.0)
            }
        }
        
        # Speichere Evaluator f√ºr weitere Verwendung
        self.evaluator = evaluator
        
        print(f"   üéØ AUC: {auc_score:.3f}")
        
        # üîß NEU: Training-Zeitr√§ume in Metriken speichern
        training_metrics = {
            'accuracy': report['accuracy'],
            'precision': report.get('1', {}).get('precision', 0.0),
            'recall': report.get('1', {}).get('recall', 0.0),
            'f1_score': report.get('1', {}).get('f1-score', 0.0),
            'auc': auc_score,
            'training_from': training_from,
            'training_to': training_to,
            'prediction_timebase': prediction_timebase,
            'samples_count': len(X),
            'features_count': len(self.feature_names),
            'churn_rate': y.mean(),
            'model_type': 'Enhanced_EarlyWarning'
        }
        
        print(f"‚úÖ Training abgeschlossen: {len(X)} Samples, AUC: {auc_score:.3f}")
        
        return training_metrics
    
    def _load_optimized_config(self):
        """L√§dt optimierte Konfiguration ‚Äì bevorzugt Experiment-Override, sonst Datei."""
        # 1) Experiment-Override (vom Orchestrator gesetzt)
        try:
            override = getattr(self, '_optimized_config_override', None)
            if isinstance(override, dict) and override:
                print("üß© Algorithm Config Source: experiments.hyperparameters.algorithm_config (override)")
                return override
        except Exception:
            pass

        # 2) Fallback: Datei aus config/
        try:
            config_path = self.paths.config_directory() / "algorithm_config_optimized.json"
            if config_path.exists():
                with open(config_path, 'r', encoding='utf-8') as f:
                    cfg = json.load(f)
                print("üß© Algorithm Config Source: config/algorithm_config_optimized.json")
                return cfg
            else:
                print("‚ö†Ô∏è Keine optimierte Konfiguration gefunden")
                return None
        except Exception as e:
            print(f"‚ö†Ô∏è Fehler beim Laden der optimierten Konfiguration: {e}")
            return None
    
    def _create_optimized_model(self, config):
        """Erstellt Modell basierend auf optimierter Konfiguration"""
        model_type = config.get('model_type', 'RandomForestClassifier')
        
        if model_type == 'RandomForestClassifier':
            return RandomForestClassifier(
                n_estimators=config.get('n_estimators', 200),
                max_depth=config.get('max_depth', 20),
                min_samples_split=config.get('min_samples_split', 5),
                min_samples_leaf=config.get('min_samples_leaf', 1),
                random_state=config.get('random_state', 42),
                class_weight=config.get('class_weight', 'balanced_subsample'),
                n_jobs=-1
            )
        else:
            print(f"‚ö†Ô∏è Unbekannter Modell-Typ: {model_type} - verwende RandomForest")
            return RandomForestClassifier(
                n_estimators=200,
                max_depth=20,
                min_samples_split=5,
                min_samples_leaf=1,
                random_state=42,
                class_weight='balanced_subsample',
                n_jobs=-1
            )
    
    def _apply_sampling(self, X_train, y_train, sampling_config):
        """Wendet Sampling basierend auf optimierter Konfiguration an"""
        strategy = sampling_config.get('strategy', 'none')
        parameters = sampling_config.get('parameters', {})
        
        # üîç IDENTIFIZIERE KATEGORISCHE FEATURES (One-Hot-encoded)
        categorical_features = []
        for col in X_train.columns:
            # One-Hot-encoded Features erkennen (enthalten '_' und sind bin√§r)
            if '_' in col and X_train[col].nunique() <= 2:
                categorical_features.append(col)
        
        if categorical_features:
            print(f"üîç Erkannte {len(categorical_features)} kategorische Features - werden vom Sampling ausgeschlossen")
            print(f"   Kategorische Features: {categorical_features[:5]}...")
        
        # üö´ SCHLIESSE KATEGORISCHE FEATURES VOM SAMPLING AUS
        if categorical_features:
            # Trenne kategorische und numerische Features
            X_categorical = X_train[categorical_features]
            X_numerical = X_train.drop(categorical_features, axis=1)
            
            # Wende Sampling nur auf numerische Features an
            if strategy == 'smote_enhanced':
                from imblearn.over_sampling import SMOTE
                sampling_ratio = parameters.get('sampling_ratio', 3.0)
                if sampling_ratio > 1.0:
                    sampling_strategy = 1.0
                else:
                    sampling_strategy = sampling_ratio
                sampler = SMOTE(
                    sampling_strategy=sampling_strategy,
                    k_neighbors=parameters.get('k_neighbors', 3),
                    random_state=parameters.get('random_state', 42)
                )
            elif strategy == 'smoteenn':
                from imblearn.combine import SMOTEENN
                sampler = SMOTEENN(random_state=parameters.get('random_state', 42))
            else:
                print(f"‚ö†Ô∏è Unbekannte Sampling-Strategie: {strategy} - keine Sampling angewendet")
                return X_train, y_train
            
            # Sampling nur auf numerische Features
            X_numerical_resampled, y_resampled = sampler.fit_resample(X_numerical, y_train)
            
            # F√ºge kategorische Features wieder hinzu (ohne Sampling)
            # F√ºr neue Samples (SMOTE-generiert): Verwende 0 f√ºr kategorische Features
            if len(y_resampled) > len(y_train):
                # SMOTE hat neue Samples erstellt
                # Verwende die Indizes von X_numerical_resampled f√ºr Konsistenz
                X_categorical_resampled = pd.DataFrame(0, index=X_numerical_resampled.index, columns=X_categorical.columns)
            else:
                # Keine neuen Samples - verwende urspr√ºngliche Indizes
                X_categorical_resampled = X_categorical.iloc[y_resampled.index]
            
            # Kombiniere numerische und kategorische Features
            X_resampled = pd.concat([X_numerical_resampled, X_categorical_resampled], axis=1)
            
            # Stelle sicher, dass die Feature-Reihenfolge mit dem urspr√ºnglichen Training √ºbereinstimmt
            # Verwende die urspr√ºngliche Reihenfolge der Features
            original_columns = X_train.columns.tolist()
            X_resampled = X_resampled[original_columns]
            
            return X_resampled, y_resampled
        else:
            # Keine kategorischen Features - normales Sampling
            if strategy == 'smote_enhanced':
                from imblearn.over_sampling import SMOTE
                sampling_ratio = parameters.get('sampling_ratio', 3.0)
                if sampling_ratio > 1.0:
                    sampling_strategy = 1.0
                else:
                    sampling_strategy = sampling_ratio
                sampler = SMOTE(
                    sampling_strategy=sampling_strategy,
                    k_neighbors=parameters.get('k_neighbors', 3),
                    random_state=parameters.get('random_state', 42)
                )
            elif strategy == 'smoteenn':
                from imblearn.combine import SMOTEENN
                sampler = SMOTEENN(random_state=parameters.get('random_state', 42))
            else:
                print(f"‚ö†Ô∏è Unbekannte Sampling-Strategie: {strategy} - keine Sampling angewendet")
                return X_train, y_train
            
            return sampler.fit_resample(X_train, y_train)
    
    def real_world_backtest(self, training_from=None, training_to=None, test_from=None, test_to=None, use_existing_model=False):
        """üèÜ REAL-WORLD BACKTEST mit flexiblen Zeitr√§umen und echten Performance-Metriken"""
        
        if training_from is None or training_to is None or test_from is None or test_to is None:
            raise ValueError("‚ùå FEHLER: Alle Zeitr√§ume m√ºssen explizit gesetzt werden! Keine Fallbacks erlaubt.")
        
        # üîß DATENTYP-KONVERTIERUNG: Konvertiere alle Perioden zu int
        training_from = int(training_from)
        training_to = int(training_to)
        test_from = int(test_from)
        test_to = int(test_to)
        
        print(f"\nüèÜ === REAL-WORLD BACKTEST (Flexible Zeitr√§ume) ===")
        print(f"üìÖ Training: {training_from} bis {training_to}")
        print(f"üìÖ Test: {test_from} bis {test_to}")
        
        # Dynamische Spaltennamen aus Data Dictionary
        primary_key_col, timebase_col, target_col = self._get_dynamic_column_names()
        
        # Pr√ºfe ob Features bereits verf√ºgbar sind
        if hasattr(self, 'features_df') and self.features_df is not None:
            print("‚úÖ Features bereits verf√ºgbar - pr√ºfe auf Early Warning Features...")
            # Pr√ºfe ob Early Warning Features vorhanden sind
            ew_features = [col for col in self.features_df.columns if '_ew_' in col]
            if len(ew_features) == 0:
                print("üö® Keine Early Warning Features gefunden - erstelle Features neu...")
                self.load_and_enhance_features()
            else:
                print(f"‚úÖ {len(ew_features)} Early Warning Features gefunden")
        else:
            print("üîÑ Features nicht verf√ºgbar - erstelle Features neu...")
            self.load_and_enhance_features()
        
        # Erstelle Customer-Level Dataset
        customer_df = self.create_customer_dataset(prediction_timebase=test_from)
        
        # Training Data (historische Daten)
        train_df = customer_df.copy()
        # ‚úÖ timebase_col ist bereits NUMERIC (aus Data Dictionary)
        train_data = train_df[
            (train_df[timebase_col] >= training_from) &
            (train_df[timebase_col] <= training_to)
        ]
        
        print(f"üìö Training Phase ({training_from} bis {training_to}):")
        print(f"   Kunden: {len(train_data):,}")
        print(f"   Churn Rate: {train_data[target_col].mean():.3f}")
        
        # Test Data (zuk√ºnftige Daten - DYNAMISCHES FENSTER basierend auf Test-Perioden-L√§nge)
        # Korrekte Berechnung f√ºr YYYYMM Format
        def months_between(yyyymm1, yyyymm2):
            """Berechnet Monate zwischen zwei YYYYMM Perioden"""
            year1 = yyyymm1 // 100
            month1 = yyyymm1 % 100
            year2 = yyyymm2 // 100
            month2 = yyyymm2 % 100
            
            # Berechne Monate zwischen den beiden Daten
            months_diff = (year2 - year1) * 12 + (month2 - month1)
            return months_diff + 1  # +1 weil wir beide Endpunkte einschlie√üen
        
        # Korrekte Test-Perioden-L√§nge f√ºr YYYYMM Format
        test_period_length = months_between(test_from, test_to)
        
        # DYNAMISCHE TARGET-DEFINITION: Anpassung an Test-Perioden-L√§nge
        # KORREKTUR: Target-Fenster ist BEGRENZT auf Test-Perioden-L√§nge
        target_window_months = min(test_period_length, 12)  # Maximal 12 Monate, aber nicht l√§nger als Test-Periode
        target_start = test_from
        
        print(f"üéØ DYNAMISCHE TARGET-DEFINITION:")
        print(f"   Test-Perioden-L√§nge: {test_period_length} Monate")
        print(f"   Target-Fenster: {target_window_months} Monate (ab {target_start})")
        
        # Finde alle Kunden die in Test-Periode aktiv waren
        # Korrekte Test-Perioden f√ºr YYYYMM Format
        def generate_yyyymm_periods(start_yyyymm, end_yyyymm):
            """Generiert Liste von YYYYMM Perioden zwischen start und end"""
            periods = []
            current = start_yyyymm
            while current <= end_yyyymm:
                periods.append(current)
                # N√§chste Periode
                year = current // 100
                month = current % 100
                if month == 12:
                    current = (year + 1) * 100 + 1
                else:
                    current = year * 100 + month + 1
            return periods
        
        # Berechne Target Window Information
        target_window_info = self._calculate_target_window_info(test_from, test_to)
        test_periods = target_window_info['test_periods']
        # ‚úÖ timebase_col ist bereits NUMERIC (aus Data Dictionary)
        features_df_numeric = self.features_df.copy()
        
        # KONVERTIERUNG: Verarbeite ALLE Kunden die im Test-Zeitraum vorhanden waren
        # Nicht nur aktive Kunden, sondern alle Kunden die im Test-Zeitraum vorhanden waren
        test_period_customers = features_df_numeric[
            (features_df_numeric[timebase_col] == test_from)  # Kunden im Test-Zeitraum
        ][primary_key_col].unique()
        
        print(f"üéØ Verarbeite ALLE Kunden die im Test-Zeitraum aktiv waren: {len(test_period_customers)} Kunden")
        
        # Verwende alle Test-Zeitraum-Kunden f√ºr Vorhersagen
        test_customers = test_period_customers
        
        # Erstelle Test-Dataset mit Customer-Level Aggregation
        test_records = []
        
        for customer in test_customers:
            # Historische Daten f√ºr Features (bis Training-Ende)
            # ‚úÖ timebase_col ist bereits NUMERIC (aus Data Dictionary)
            customer_historical = self.features_df[
                (self.features_df[primary_key_col] == customer) &
                (self.features_df[timebase_col] <= training_to)
            ].copy()
            
            # Zuk√ºnftige Daten f√ºr Target (BEGRENZT auf Test-Perioden)
            customer_future = self.features_df[
                (self.features_df[primary_key_col] == customer) &
                (self.features_df[timebase_col] >= target_start) &  # Ab Test-Start
                (self.features_df[timebase_col] <= test_to)  # Bis Test-Ende (nicht dar√ºber hinaus!)
            ].copy()
            
            if len(customer_historical) > 0:
                # Nehm den letzten historischen Record f√ºr Features
                latest_record = customer_historical.iloc[-1].copy()
                
                # Churn-Status in Zukunft (Dynamisches Fenster)
                will_churn = (~customer_future[target_col]).any()
                
                # Zeit bis Churn (f√ºr Analyse) - Korrekte YYYYMM Berechnung
                if will_churn:
                    churn_period = customer_future[~customer_future[target_col]][timebase_col].min()
                    months_to_churn = months_between(target_start, churn_period) - 1  # -1 weil wir ab target_start z√§hlen
                else:
                    months_to_churn = None
                
                latest_record['ACTUAL_CHURN'] = int(will_churn)
                latest_record['MONTHS_TO_CHURN'] = months_to_churn
                test_records.append(latest_record)
        
        test_df = pd.DataFrame(test_records)
        
        print(f"üß™ Test Phase - {target_window_months}-Monate-Fenster (>= {target_start}):")
        print(f"   Kunden: {len(test_df):,}")
        print(f"   Actual Churn Rate ({target_window_months}+ Monate): {test_df['ACTUAL_CHURN'].mean():.3f}")
        
        # Zeit-zu-Churn Analyse
        churn_timing = test_df[test_df['ACTUAL_CHURN'] == 1]['MONTHS_TO_CHURN'].dropna()
        if len(churn_timing) > 0:
            print(f"   Durchschnittliche Zeit bis Churn: {churn_timing.mean():.1f} Monate")
            
            # Churn-Verteilung
            early_churns = len(churn_timing[churn_timing <= 3])
            mid_churns = len(churn_timing[(churn_timing > 3) & (churn_timing <= 6)])
            late_churns = len(churn_timing[churn_timing > 6])
            print(f"   Churn-Verteilung: 0-3M: {early_churns}, 4-6M: {mid_churns}, 6+M: {late_churns}")
            
            # MONTHS_TO_CHURN Wahrscheinlichkeits-Analyse
            months_to_churn_analysis = {
                'mean_months_to_churn': float(churn_timing.mean()),
                'median_months_to_churn': float(churn_timing.median()),
                'min_months_to_churn': float(churn_timing.min()),
                'max_months_to_churn': float(churn_timing.max()),
                'std_months_to_churn': float(churn_timing.std()),
                'months_distribution': churn_timing.value_counts().sort_index().to_dict(),
                'probability_categories': {
                    'immediate': len(churn_timing[churn_timing <= 1]) / len(churn_timing),
                    'short_term': len(churn_timing[(churn_timing > 1) & (churn_timing <= 3)]) / len(churn_timing),
                    'medium_term': len(churn_timing[(churn_timing > 3) & (churn_timing <= 6)]) / len(churn_timing),
                    'long_term': len(churn_timing[(churn_timing > 6) & (churn_timing <= 12)]) / len(churn_timing),
                    'very_long_term': len(churn_timing[churn_timing > 12]) / len(churn_timing)
                }
            }
            
            print(f"\nüéØ MONTHS_TO_CHURN WAHRSCHEINLICHKEITEN:")
            print(f"   Sofort (0-1 Monate): {months_to_churn_analysis['probability_categories']['immediate']:.1%}")
            print(f"   Kurzfristig (2-3 Monate): {months_to_churn_analysis['probability_categories']['short_term']:.1%}")
            print(f"   Mittelfristig (4-6 Monate): {months_to_churn_analysis['probability_categories']['medium_term']:.1%}")
            print(f"   Langfristig (7-12 Monate): {months_to_churn_analysis['probability_categories']['long_term']:.1%}")
            print(f"   Sehr langfristig (>12 Monate): {months_to_churn_analysis['probability_categories']['very_long_term']:.1%}")
        else:
            months_to_churn_analysis = {}
        
        # INTELLIGENTE MODELL-WAHL: Verwende IMMER das bereits trainierte Modell f√ºr Backtest
        if self.model is not None:
            print("üéØ Verwende bereits trainiertes Modell f√ºr Backtest")
            backtest_model = self.model
            model_training_performed = False
            
            # ‚úÖ KORRIGIERT: Definiere X_train/y_train auch f√ºr bereits trainiertes Modell
            X_train = train_data[self.feature_names].fillna(0)
            y_train = train_data[target_col]
        else:
            print("‚ö†Ô∏è Kein trainiertes Modell verf√ºgbar - trainiere neues Modell")
            # ‚úÖ MIGRIERT: Fallback Model Training mit ChurnModelTrainer
            print("üîß Fallback Model Training mit ChurnModelTrainer...")
            X_train = train_data[self.feature_names].fillna(0)
            y_train = train_data[target_col]
            
            # Initialisiere Fallback Model Trainer
            fallback_trainer = ChurnModelTrainer(experiment_id=getattr(self, 'experiment_id', None))
            
            # Standard Parameter f√ºr Fallback (IDENTICAL zu ChurnModelTrainer!)
            fallback_params = {
                'n_estimators': 300,
                'max_depth': 12,
                'min_samples_split': 5,     # ‚úÖ KRITISCHER PARAMETER HINZUGEF√úGT!
                'min_samples_leaf': 2,      # ‚úÖ KRITISCHER PARAMETER HINZUGEF√úGT!
                'random_state': 42,
                'class_weight': 'balanced',
                'n_jobs': -1
            }
            
            # Trainiere Fallback Model
            backtest_model = fallback_trainer.train_random_forest(
                X_train.values, y_train.values,
                params=fallback_params
            )
            model_training_performed = True
        
        # ‚úÖ MIGRIERT: Training Performance mit ChurnEvaluator
        print(f"\nüìö TRAINING PERFORMANCE (mit ChurnEvaluator):")
        
        # Initialisiere Training Evaluator
        training_evaluator = ChurnEvaluator(experiment_id=getattr(self, 'experiment_id', None))
        
        # Training Evaluation mit ChurnEvaluator
        training_evaluation = training_evaluator.evaluate_model_performance(
            backtest_model,
            X_train.values, y_train.values,
            feature_names=self.feature_names
        )
        
        print(f"   üéØ Training AUC: {training_evaluation.get('auc', 0.0):.3f}")
        print(f"   üéØ Training Precision: {training_evaluation.get('precision', 0.0):.3f}")
        print(f"   üéØ Training Recall: {training_evaluation.get('recall', 0.0):.3f}")
        print(f"   üéØ Training F1-Score: {training_evaluation.get('f1_score', 0.0):.3f}")
        print(f"   üéØ Training Accuracy: {training_evaluation.get('accuracy', 0.0):.3f}")
        
        # Kompatibilit√§t: Legacy Variablen f√ºr Backward Compatibility
        train_auc = training_evaluation.get('auc', 0.0)
        train_report = {
            'accuracy': training_evaluation.get('accuracy', 0.0),
            '1': {
                'precision': training_evaluation.get('precision', 0.0),
                'recall': training_evaluation.get('recall', 0.0),
                'f1-score': training_evaluation.get('f1_score', 0.0)
            }
        }
        
        # Speichere Training-KPIs
        training_metrics = {
            'training_accuracy': train_report['accuracy'],
            'training_precision': train_report['1']['precision'] if '1' in train_report else 0.0,
            'training_recall': train_report['1']['recall'] if '1' in train_report else 0.0,
            'training_f1_score': train_report['1']['f1-score'] if '1' in train_report else 0.0,
            'training_auc': train_auc,
            'training_samples': len(train_data)
        }
        
        # Predictions
        X_test = test_df[self.feature_names].fillna(0)
        y_test = test_df['ACTUAL_CHURN']
        
        y_pred = backtest_model.predict(X_test)
        y_pred_proba_full = backtest_model.predict_proba(X_test)
        
        # Sichere Behandlung von predict_proba (falls nur eine Klasse)
        if y_pred_proba_full.shape[1] > 1:
            y_pred_proba = y_pred_proba_full[:, 1]  # Churn-Wahrscheinlichkeit
        else:
            # Fallback: Nur eine Klasse vorhergesagt
            y_pred_proba = y_pred_proba_full[:, 0]
            print("‚ö†Ô∏è Backtest: Modell hat nur eine Klasse vorhergesagt - verwende Fallback")
        
        # ‚úÖ MIGRIERT: Backtest Metrics mit ChurnEvaluator
        print(f"\nüß™ BACKTEST EVALUATION (mit ChurnEvaluator):")
        
        # Initialisiere Backtest Evaluator
        backtest_evaluator = ChurnEvaluator(experiment_id=getattr(self, 'experiment_id', None))
        
        # Backtest Evaluation mit ChurnEvaluator
        backtest_evaluation = backtest_evaluator.evaluate_model_performance(
            backtest_model,
            X_test.values, y_test.values,
            feature_names=self.feature_names
        )
        
        print(f"   üéØ Backtest AUC: {backtest_evaluation.get('auc', 0.0):.3f}")
        print(f"   üéØ Backtest Precision: {backtest_evaluation.get('precision', 0.0):.3f}")
        print(f"   üéØ Backtest Recall: {backtest_evaluation.get('recall', 0.0):.3f}")
        print(f"   üéØ Backtest F1-Score: {backtest_evaluation.get('f1_score', 0.0):.3f}")
        
        # Kompatibilit√§t: Legacy Variablen
        backtest_auc = backtest_evaluation.get('auc', 0.0)
        backtest_report = {
            'accuracy': backtest_evaluation.get('accuracy', 0.0),
            '1': {
                'precision': backtest_evaluation.get('precision', 0.0),
                'recall': backtest_evaluation.get('recall', 0.0),
                'f1-score': backtest_evaluation.get('f1_score', 0.0)
            }
        }
        
        # Risk Segmentation
        test_df['CHURN_PROBABILITY'] = y_pred_proba
        
        # Wissenschaftlich fundierte Schwellwert-Optimierung
        print("\nüî¨ WISSENSCHAFTLICHE SCHWELLWERT-OPTIMIERUNG:")
        optimizer = RobustThresholdOptimizer(cv_folds=5, n_bootstrap=1000)
        threshold_result = optimizer.optimize(y_test, y_pred_proba)
        
        # Extrahiere optimalen Schwellwert aus Dictionary
        if isinstance(threshold_result, dict):
            optimal_threshold = threshold_result.get('optimal_threshold', 0.5)
            method = threshold_result.get('method', 'unknown')
            print(f"   Optimaler Schwellwert: {optimal_threshold:.3f}")
            print(f"   Methode: {method}")
        else:
            optimal_threshold = threshold_result
            print(f"   Optimaler Schwellwert: {optimal_threshold:.3f}")
        
        # GMM-basierte Risiko-Segmentierung auf Testdaten
        print("\nüî¨ GMM-BASIERTE RISIKO-SEGMENTIERUNG:")
        test_risk_levels, test_cluster_stats = self._calculate_gmm_risk_segments(y_pred_proba, optimal_threshold, "Test")
        test_df['RISK_LEVEL'] = test_risk_levels
        
        # GMM-basierte Risiko-Segmentierung auf Trainingsdaten
        y_train_pred_proba = backtest_model.predict_proba(X_train)[:, 1]
        train_risk_levels, train_cluster_stats = self._calculate_gmm_risk_segments(y_train_pred_proba, optimal_threshold, "Training")
        
        # Cluster-Vergleich
        cluster_comparison = self._compare_train_test_clusters(train_cluster_stats, test_cluster_stats)
        
        risk_analysis = test_df.groupby('RISK_LEVEL').agg({
            'ACTUAL_CHURN': ['count', 'sum', 'mean']
        }).round(3)
        
        print(f"\nüèÜ BACKTEST RESULTS:")
        print(f"   üéØ Accuracy: {backtest_report['accuracy']:.3f}")
        
        # Sichere Pr√ºfung f√ºr Klasse '1' (Churn)
        if '1' in backtest_report:
            print(f"   üéØ Precision: {backtest_report['1']['precision']:.3f}")
            print(f"   üéØ Recall: {backtest_report['1']['recall']:.3f}")
            print(f"   üéØ F1-Score: {backtest_report['1']['f1-score']:.3f}")
        else:
            print(f"   ‚ö†Ô∏è Keine Churn-Klasse '1' gefunden - Modell hat nur eine Klasse vorhergesagt")
            print(f"   üéØ Precision: N/A")
            print(f"   üéØ Recall: N/A")
            print(f"   üéØ F1-Score: N/A")
        
        print(f"   üéØ AUC: {backtest_auc:.3f}")
        
        print(f"\nüìä GMM-BASIERTE RISIKO-SEGMENTIERUNG - {target_window_months}-Monate-Fenster:")
        print(f"   Wissenschaftlich optimierter Schwellwert: {optimal_threshold:.3f}")
        
        # Zeige Test-Cluster-Statistiken
        print(f"\nüéØ TEST-CLUSTER (Backtest {test_from}-{test_to}):")
        for risk_level in test_cluster_stats['risk_levels']:
            if risk_level in risk_analysis.index:
                customers = risk_analysis.loc[risk_level, ('ACTUAL_CHURN', 'count')]
                churns = risk_analysis.loc[risk_level, ('ACTUAL_CHURN', 'sum')]
                rate = risk_analysis.loc[risk_level, ('ACTUAL_CHURN', 'mean')]
                cluster_mean = test_cluster_stats['cluster_means'].get(risk_level, 0)
                print(f"   {risk_level}: {customers} customers, {churns} churns, {rate:.1%} rate, Mean Prob: {cluster_mean:.3f}")
        
        # Zeige Training-Cluster-Statistiken
        print(f"\nüìö TRAINING-CLUSTER (Training {training_from}-{training_to}):")
        for risk_level in train_cluster_stats['risk_levels']:
            cluster_mean = train_cluster_stats['cluster_means'].get(risk_level, 0)
            cluster_size = train_cluster_stats['cluster_sizes'].get(risk_level, 0)
            print(f"   {risk_level}: {cluster_size} customers, Mean Prob: {cluster_mean:.3f}")
        
        # Zeige Cluster-Vergleich
        print(f"\nüîÑ CLUSTER-STABILIT√ÑT:")
        print(f"   Training Cluster: {len(train_cluster_stats['risk_levels'])}")
        print(f"   Test Cluster: {len(test_cluster_stats['risk_levels'])}")
        print(f"   Stabilit√§t: {cluster_comparison['stability_score']:.3f}")
        
        # INTELLIGENTES SPEICHERN: Speichere IMMER die echten Backtest-Performance-Metriken
        print(f"\nüíæ Speichere Enhanced Early Warning Modell mit ECHTEN Backtest-Performance-Metriken...")
        
        # Sichere Pr√ºfung f√ºr Klasse '1' (Churn)
        if '1' in backtest_report:
            print(f"‚úÖ ECHTE PERFORMANCE: AUC {backtest_auc:.3f}, Recall {backtest_report['1']['recall']:.3f}")
            
                    # ECHTE Backtest-Metriken als Hauptmetriken
            real_performance_metrics = {
                'precision': backtest_report['1']['precision'],
                'recall': backtest_report['1']['recall'],
                'f1_score': backtest_report['1']['f1-score'],
                'roc_auc': backtest_auc,
                'accuracy': backtest_report['accuracy'],
                # Training-KPIs hinzuf√ºgen
                'training_accuracy': training_metrics['training_accuracy'],
                'training_precision': training_metrics['training_precision'],
                'training_recall': training_metrics['training_recall'],
                'training_f1_score': training_metrics['training_f1_score'],
                'training_auc': training_metrics['training_auc'],
                'training_samples': training_metrics['training_samples']
            }
        else:
            print(f"‚ö†Ô∏è Keine Churn-Klasse '1' gefunden - verwende Standard-Metriken")
            
            # Fallback-Metriken
            real_performance_metrics = {
                'precision': 0.0,
                'recall': 0.0,
                'f1_score': 0.0,
                'roc_auc': backtest_auc,
                'accuracy': backtest_report['accuracy']
            }
        
        training_info = {
            "algorithm": "RandomForest_Enhanced_Backtest",
            "level": "enhanced_early_warning_backtest",
            "features_count": len(self.feature_names),
            "compatibility": "system_health_ui",
            "lead_time_months": target_window_months,
            "warning_signals": len([f for f in self.feature_names if '_ew_' in f]),
            "training_period": f"{training_from}-{training_to}",
            "test_period": f"{test_from}-{test_to}",
            "target_window_months": target_window_months,
            "target_start": target_start,
            "training_customers": len(train_data),
            "test_customers": len(test_df),
            "performance_type": "real_world_backtest",
            "note": f"Echte Performance aus {target_window_months}-Monate-Fenster Backtest"
        }
        
        # Konvertiere training_info zu dict falls es ein tuple ist
        if isinstance(training_info, tuple):
            training_info = dict(training_info)
        
        model_path, metadata_path = self.save_enhanced_model(
            backtest_model, 
            real_performance_metrics, 
            self.feature_names,
            training_info
        )
        
        # Feature Importance f√ºr Erkl√§rungen
        feature_importance = pd.DataFrame({
            'feature': self.feature_names,
            'importance': backtest_model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        # Erstelle zwei verschiedene Kunden-Listen f√ºr UI
        
        # 1. VALIDATION CUSTOMERS: ALLE Test-Zeitraum-Kunden mit Churn-Vorhersagen
        validation_customers = []
        print(f"üéØ Erstelle Customer Predictions f√ºr alle {len(test_customers)} Test-Zeitraum-Kunden...")
        
        # Erstelle Vorhersagen f√ºr ALLE Test-Zeitraum-Kunden (nicht nur test_df)
        test_period_data = []
        print(f"üîç Analysiere {len(test_customers)} Test-Zeitraum-Kunden...")
        
        customers_with_historical = 0
        customers_without_historical = 0
        
        for customer in test_customers:
            # Historische Daten f√ºr Features (bis Training-Ende)
            customer_historical = self.features_df[
                (self.features_df[primary_key_col] == customer) &
                (self.features_df[timebase_col] <= training_to)
            ].copy()
            
            if len(customer_historical) > 0:
                customers_with_historical += 1
                # Nehm den letzten historischen Record f√ºr Features
                latest_record = customer_historical.iloc[-1].copy()
                
                # Zuk√ºnftige Daten f√ºr Target (BEGRENZT auf Test-Perioden)
                customer_future = self.features_df[
                    (self.features_df[primary_key_col] == customer) &
                    (self.features_df[timebase_col] >= target_start) &  # Ab Test-Start
                    (self.features_df[timebase_col] <= test_to)  # Bis Test-Ende
                ].copy()
                
                # Churn-Status in Zukunft (Dynamisches Fenster)
                will_churn = (~customer_future[target_col]).any() if len(customer_future) > 0 else False
                
                latest_record['ACTUAL_CHURN'] = int(will_churn)
                test_period_data.append(latest_record)
            else:
                customers_without_historical += 1
                # ENDG√úLTIGE L√ñSUNG: Erstelle Dummy-Record f√ºr Kunden ohne historische Daten
                # Verwende Durchschnittswerte f√ºr Features
                dummy_record = {}
                for feature in self.feature_names:
                    # Verwende Median-Werte aus dem gesamten Dataset
                    feature_median = self.features_df[feature].median()
                    dummy_record[feature] = feature_median
                
                # Setze Primary Key
                dummy_record[primary_key_col] = customer
                
                # Zuk√ºnftige Daten f√ºr Target
                customer_future = self.features_df[
                    (self.features_df[primary_key_col] == customer) &
                    (self.features_df[timebase_col] >= target_start) &
                    (self.features_df[timebase_col] <= test_to)
                ].copy()
                
                # Churn-Status in Zukunft
                will_churn = (~customer_future[target_col]).any() if len(customer_future) > 0 else False
                dummy_record['ACTUAL_CHURN'] = int(will_churn)
                
                # Konvertiere dict zu pandas Series f√ºr konsistente Datenstruktur
                dummy_series = pd.Series(dummy_record)
                test_period_data.append(dummy_series)
        
        print(f"üìä Kunden-Analyse:")
        print(f"   Mit historischen Daten: {customers_with_historical}")
        print(f"   Ohne historische Daten: {customers_without_historical}")
        print(f"   Gesamt verarbeitet: {len(test_period_data)}")
        
        # SAUBERE L√ñSUNG: Konvertiere test_period_data zu DataFrame und erstelle validation_customers
        print(f"üéØ SAUBERE L√ñSUNG: Konvertiere {len(test_period_data)} Test-Period-Daten zu DataFrame")
        
        # Konvertiere test_period_data zu DataFrame
        if len(test_period_data) > 0:
            test_period_df = pd.DataFrame(test_period_data)
            print(f"‚úÖ DataFrame erstellt mit {len(test_period_df)} Kunden und {len(test_period_df.columns)} Features")
            
            # Stelle sicher, dass alle ben√∂tigten Features vorhanden sind
            missing_features = set(self.feature_names) - set(test_period_df.columns)
            if missing_features:
                print(f"‚ö†Ô∏è Fehlende Features erg√§nzt: {len(missing_features)} Features")
                for feature in missing_features:
                    test_period_df[feature] = self.features_df[feature].median()
            
            # Erstelle X_test f√ºr Vorhersagen
            X_test = test_period_df[self.feature_names].fillna(0)
            y_test = test_period_df['ACTUAL_CHURN']
            
            # Vorhersagen f√ºr alle Test-Kunden
            y_pred_proba_full = backtest_model.predict_proba(X_test)
            
            # Sichere Behandlung von predict_proba (falls nur eine Klasse)
            if y_pred_proba_full.shape[1] > 1:
                y_pred_proba = y_pred_proba_full[:, 1]  # Churn-Wahrscheinlichkeit
            else:
                # Fallback: Nur eine Klasse vorhergesagt
                y_pred_proba = y_pred_proba_full[:, 0]
                print("‚ö†Ô∏è Modell hat nur eine Klasse vorhergesagt - verwende Fallback")
            
            # Erstelle validation_customers aus test_period_df
            validation_customers = []
            for i, (idx, row) in enumerate(test_period_df.iterrows()):
                # Engineered Features f√ºr diesen Kunden extrahieren
                try:
                    import numpy as np
                    engineered = {}
                    for f in self.feature_names:
                        if f in row.index:
                            v = row[f]
                            if pd.isna(v):
                                engineered[f] = None
                            elif isinstance(v, (np.generic,)):
                                try:
                                    engineered[f] = v.item()
                                except Exception:
                                    engineered[f] = float(v)
                            else:
                                engineered[f] = v
                except Exception:
                    engineered = {}

                customer_data = {
                    primary_key_col: row[primary_key_col],
                    'risk_level': 'HIGH' if y_pred_proba[i] > optimal_threshold else 'LOW',
                    'churn_probability': y_pred_proba[i],
                    'actual_churn': int(row['ACTUAL_CHURN']),
                    'predicted_churn': int(y_pred_proba[i] > 0.5),
                    'customer_type': 'validation',
                    'engineered_features': engineered
                }
                validation_customers.append(customer_data)
            
            print(f"‚úÖ SAUBERE L√ñSUNG: {len(validation_customers)} validation_customers aus test_period_df erstellt")
        else:
            print("‚ùå Keine test_period_data verf√ºgbar - kann keine validation_customers erstellen")
            validation_customers = []
            test_period_df = pd.DataFrame()
        
        print(f"‚úÖ Customer Predictions erstellt: {len(validation_customers)} Kunden")
        
        # 2. ACTIVE CUSTOMERS: Aktuelle Kunden f√ºr Risk Level Action Table
        # Nutze die Test-Perioden f√ºr aktive Kunden (mehr Daten verf√ºgbar)
        print(f"\nüìä Erstelle aktive Kunden-Liste f√ºr Risk Level Action Table...")
        
        # Finde alle aktiven Kunden nach dem Backtest-Zeitraum
        # Verwende backtest_to Parameter statt letzten verf√ºgbaren Monat
        active_timebase = test_to  # Verwende den backtest_to Parameter
        print(f"üéØ Suche aktive Kunden nach dem Backtest-Zeitraum: {active_timebase}")
        
        # Finde alle Kunden, die nach dem Backtest-Zeitraum noch aktiv waren
        active_customers_data = self.features_df[
            (self.features_df['I_TIMEBASE'] == active_timebase) & 
            (self.features_df[self.get_target_column_name()] == True)
        ].copy()
        
        # Gruppiere nach Primary Key und nehme den letzten verf√ºgbaren Record
        active_customers_data = active_customers_data.sort_values(timebase_col)
        active_customers_data = active_customers_data.groupby(primary_key_col).last().reset_index()
        
        print(f"‚úÖ {len(active_customers_data)} aktive Kunden nach dem Backtest-Zeitraum ({active_timebase}) gefunden")
        
        # Erstelle Vorhersagen f√ºr aktive Kunden
        active_customers = []
        if len(active_customers_data) > 0:
            X_active = active_customers_data[self.feature_names].fillna(0)
            y_pred_proba_full_active = backtest_model.predict_proba(X_active)
            
            # Sichere Behandlung von predict_proba (falls nur eine Klasse)
            if y_pred_proba_full_active.shape[1] > 1:
                y_pred_proba_active = y_pred_proba_full_active[:, 1]  # Churn-Wahrscheinlichkeit
            else:
                # Fallback: Nur eine Klasse vorhergesagt
                y_pred_proba_active = y_pred_proba_full_active[:, 0]
                print("‚ö†Ô∏è Active Customers: Modell hat nur eine Klasse vorhergesagt - verwende Fallback")
            
            # Risk Level f√ºr aktive Kunden mit optimalem Schwellwert
            active_customers_data['CHURN_PROBABILITY'] = y_pred_proba_active
            active_customers_data['RISK_LEVEL'] = pd.cut(
                y_pred_proba_active, 
                bins=[0, optimal_threshold, 1.0], 
                labels=['LOW', 'HIGH']
            )
            
            for _, row in active_customers_data.iterrows():
                customer_data = {
                    primary_key_col: row[primary_key_col],  # Dynamischer PRIMARY_KEY aus Data Dictionary
                    'risk_level': row['RISK_LEVEL'],
                    'churn_probability': row['CHURN_PROBABILITY'],
                    'actual_churn': None,  # Unbekannt f√ºr aktive Kunden
                    'predicted_churn': int(row['CHURN_PROBABILITY'] > 0.5),
                    'customer_type': 'active'  # Marker f√ºr UI
                }
                
                # Top Features f√ºr aktive Kunden
                customer_features = {}
                for feature in self.feature_names:
                    if feature in row.index:
                        feature_value = row[feature]
                        feature_imp = feature_importance[feature_importance['feature'] == feature]['importance'].iloc[0]
                        customer_features[feature] = {
                            'value': feature_value, 
                            'importance': feature_imp,
                            'weighted_score': feature_value * feature_imp
                        }
                
                # Top 5 Features
                top_features = sorted(customer_features.items(), 
                                    key=lambda x: x[1]['weighted_score'], reverse=True)[:5]
                customer_data['top_features'] = top_features
                
                # Early Warning Signals
                early_warning_signals = []
                for feature, data in customer_features.items():
                    if '_ew_' in feature and data['value'] > 0:
                        signal_description = self._explain_early_warning_signal(feature, data['value'])
                        early_warning_signals.append({
                            'feature': feature,
                            'description': signal_description,
                            'value': data['value']
                        })
                
                customer_data['early_warning_signals'] = early_warning_signals
                
                # Rohe Daten ohne Business-Intelligence
                
                active_customers.append(customer_data)
            
            print(f"‚úÖ {len(active_customers)} aktive Kunden f√ºr Risk Level Action Table vorbereitet")
        
        # GMM-basierte Risk Level Verteilung f√ºr Erkl√§rungen
        risk_segmentation = {}
        for risk_level in test_cluster_stats['risk_levels']:
            if risk_level in risk_analysis.index:
                customers = int(risk_analysis.loc[risk_level, ('ACTUAL_CHURN', 'count')])
                churns = int(risk_analysis.loc[risk_level, ('ACTUAL_CHURN', 'sum')])
                rate = float(risk_analysis.loc[risk_level, ('ACTUAL_CHURN', 'mean')])
                cluster_mean = test_cluster_stats['cluster_means'].get(risk_level, 0)
                risk_segmentation[risk_level] = {
                    'customers': customers,
                    'churns': churns,
                    'rate': rate,
                    'cluster_mean': cluster_mean
                }
        
        # Sichere Pr√ºfung f√ºr Klasse '1' (Churn) im Return
        precision = 0.0
        recall = 0.0
        f1_score = 0.0
        
        if '1' in backtest_report:
            precision = backtest_report['1']['precision']
            recall = backtest_report['1']['recall']
            f1_score = backtest_report['1']['f1-score']
        
        # NEU: Speichere Backtest-Ergebnisse mit GMM-Clustering
        self._save_backtest_results(
            test_period_df=None,  # Nicht mehr ben√∂tigt, da validation_customers direkt erstellt werden
            validation_customers=validation_customers,
            backtest_results={
                'precision': precision,
                'recall': recall,
                'f1_score': f1_score,
                'auc': backtest_auc,
                'accuracy': backtest_report['accuracy'],
                'optimal_threshold': optimal_threshold,
                'gmm_clustering': {
                    'test_cluster_stats': test_cluster_stats,
                    'train_cluster_stats': train_cluster_stats,
                    'cluster_comparison': cluster_comparison
                },
                'backtest_period': {
                    'training_from': training_from,
                    'training_to': training_to,
                    'test_from': test_from,
                    'test_to': test_to
                }
            }
        )
        
        return {
            'precision': precision,
            'recall': recall,
            'f1_score': f1_score,
            'auc': backtest_auc,
            'accuracy': backtest_report['accuracy'],
            'test_customers': len(test_df),
            'actual_churns': int(test_df['ACTUAL_CHURN'].sum()),
            'risk_segmentation': risk_segmentation,
            'gmm_clustering': {
                'test_cluster_stats': test_cluster_stats,
                'train_cluster_stats': train_cluster_stats,
                'cluster_comparison': cluster_comparison
            },
            'model_saved': model_path is not None,
            'model_path': model_path,
            'metadata_path': metadata_path,
            'validation_customers': validation_customers,
            'active_customers': active_customers,
            'feature_importance': feature_importance,
            'risk_analysis': risk_analysis,
            'target_window_info': target_window_info,
            'y_pred_proba': y_pred_proba.tolist(),  # Churn-Wahrscheinlichkeiten f√ºr Analyse
            'y_test': test_df['ACTUAL_CHURN'].tolist(),  # Echte Churn-Werte f√ºr Analyse
            'optimal_threshold': optimal_threshold,  # Optimaler Schwellwert f√ºr CSV-Export
            'customer_predictions': validation_customers,  # Alle Kunden mit Churn-Wahrscheinlichkeiten
            # NEU: Alle Test-Zeitraum-Kunden mit Features und Predictions
            'test_period_customers_data': validation_customers,  # Verwende validation_customers statt test_period_df
            'test_period_customers_count': len(validation_customers),
            'backtest_period': {
                'training_from': training_from,
                'training_to': training_to,
                'test_from': test_from,
                'test_to': test_to
            },
            'experiment_id': getattr(self, 'experiment_id', None)  # MINIMALE √ÑNDERUNG: experiment_id hinzuf√ºgen
        }
        
    def _simplified_backtest(self, training_from, training_to, test_from, test_to):
        """Vereinfachtes Backtest f√ºr existierende Modelle - VERWENDET ECHTE BACKTEST-ERGEBNISSE"""
        # GLOBALE KONFIGURATION: Erzwinge echte Backtests
        global_config = get_global_config()
        enforce_real_backtest("Simplified Backtest")
        
        print(f"‚ö° VEREINFACHTES BACKTEST: {training_from}-{training_to} ‚Üí {test_from}-{test_to}")
        
        # üîß ENTFERNT: Redundante Backtest-Aufrufe - verwende nur einen korrekten real_world_backtest
        print("üéØ Verwende real_world_backtest f√ºr vereinfachtes Backtest...")
        return self.real_world_backtest(
            training_from=training_from,
            training_to=training_to, 
            test_from=test_from,
            test_to=test_to,
            use_existing_model=True  # Verwende existierendes Modell
        )
    
    def _calculate_target_window_info(self, test_from, test_to):
        """Berechnet Target Window Information f√ºr dynamische Test-Perioden"""
        def months_between(yyyymm1, yyyymm2):
            """Berechnet Monate zwischen zwei YYYYMM Werten"""
            year1, month1 = yyyymm1 // 100, yyyymm1 % 100
            year2, month2 = yyyymm2 // 100, yyyymm2 % 100
            return (year2 - year1) * 12 + (month2 - month1)
        
        def generate_yyyymm_periods(start_yyyymm, end_yyyymm):
            """Generiert Liste von YYYYMM Perioden zwischen start und end"""
            periods = []
            current = start_yyyymm
            while current <= end_yyyymm:
                periods.append(current)
                # N√§chste Periode
                year = current // 100
                month = current % 100
                if month == 12:
                    current = (year + 1) * 100 + 1
                else:
                    current = year * 100 + month + 1
            return periods
        
        # Berechne Test-Perioden-L√§nge
        test_period_length = months_between(test_from, test_to) + 1
        
        # Target Window: Dynamisch basierend auf Test-Perioden-L√§nge
        # F√ºr l√§ngere Test-Perioden: Gr√∂√üeres Target Window
        if test_period_length <= 2:
            target_window_months = 6  # Standard 6-Monate-Fenster
        elif test_period_length <= 6:
            target_window_months = 12  # Mittleres Fenster
        else:
            target_window_months = 18  # Gro√ües Fenster f√ºr lange Test-Perioden
        
        # Target Start: Ab test_from (korrekte YYYYMM Berechnung)
        target_start = test_from
        
        # Test-Perioden: Korrekte YYYYMM Format
        test_periods = generate_yyyymm_periods(test_from, test_to)
        
        return {
            'target_window_months': target_window_months,
            'target_start': target_start,
            'test_period_length': test_period_length,
            'test_periods': test_periods
        }
    
    def _explain_early_warning_signal(self, feature_name: str, value: float) -> str:
        """Erkl√§rt Early Warning Signale in verst√§ndlicher Sprache - DATA-DICTIONARY-BASIERT"""
        
        # DATA-DICTIONARY-BASIERTE ERKL√ÑRUNGEN
        data_dictionary = self._create_integrated_data_dictionary()
        
        # Extrahiere Base Feature Name (entferne Early Warning Suffixe)
        base_feature = feature_name
        if '_ew_' in feature_name:
            base_feature = feature_name.split('_ew_')[0]
        elif '_trend_' in feature_name:
            base_feature = feature_name.split('_trend_')[0]
        elif '_volatility_' in feature_name:
            base_feature = feature_name.split('_volatility_')[0]
        elif '_pattern_' in feature_name:
            base_feature = feature_name.split('_pattern_')[0]
        
        # Hole deutsche √úbersetzung aus Data Dictionary (mit None-Check)
        if data_dictionary and 'columns' in data_dictionary:
            feature_german = data_dictionary['columns'].get(base_feature, {}).get('german_name', base_feature)
        else:
            feature_german = base_feature
        
        # Bestimme Signal-Typ basierend auf Feature-Name
        if '_ew_' in feature_name:
            signal_type = 'early_warning'
        elif '_trend_' in feature_name:
            signal_type = 'trend'
        elif '_volatility_' in feature_name:
            signal_type = 'volatility'
        elif '_pattern_' in feature_name:
            signal_type = 'pattern'
        else:
            signal_type = 'general'
        
        # Generiere kontextbewusste Erkl√§rung
        return self._generate_context_aware_explanation(signal_type, feature_german, value, base_feature)
    
    def get_risk_level_action_table(self):
        """Gibt die Risk Level Action Table f√ºr aktive Kunden zur√ºck"""
        if not hasattr(self, 'last_backtest_results') or self.last_backtest_results is None:
            return None
        
        active_customers = self.last_backtest_results.get('active_customers', [])
        if not active_customers:
            return None
        
        # Erstelle DataFrame f√ºr Risk Level Action Table
        risk_table_data = []
        
        # Lese PRIMARY_KEY dynamisch aus Data Dictionary
        primary_key_col, _, _ = self._get_dynamic_column_names()
        
        for customer in active_customers:
            risk_table_data.append({
                primary_key_col: customer.get(primary_key_col, customer.get('customer_id', 'N/A')),  # Dynamischer Data Dictionary Key
                'risk_level': customer.get('risk_level', 'N/A'),
                'churn_probability': round(float(customer.get('churn_probability', 0)) * 100, 1),
                'predicted_churn': 'Ja' if customer.get('predicted_churn') == 1 else 'Nein',
                'top_features': str(customer.get('top_features', [])),
                'early_warning_signals_count': int(len(customer.get('early_warning_signals', [])))
            })
        
        risk_table_df = pd.DataFrame(risk_table_data)
        
        # Sortiere nach Churn-Probability (h√∂chste zuerst)
        risk_table_df = risk_table_df.sort_values('churn_probability', ascending=False)
        
        return risk_table_df
    
    def _generate_context_aware_explanation(self, signal_type: str, feature_german: str, value: float, base_feature: str) -> str:
        """Generiert kontextbewusste Erkl√§rung basierend auf Data Dictionary"""
        
        try:
            # Finde das Feature im Data Dictionary (mit None-Check)
            column_info = None
            if self.data_dictionary and 'columns' in self.data_dictionary:
                for col_name, col_info in self.data_dictionary['columns'].items():
                    if col_name == base_feature:
                        column_info = col_info
                        break
            
            if not column_info:
                # Fallback wenn Feature nicht gefunden
                return self._generate_generic_signal_explanation(signal_type, feature_german, value)
            
            role = column_info.get('confirmed', {}).get('role', column_info.get('role', 'UNKNOWN'))
            
            # Rolle-spezifische Erkl√§rungen
            if role == 'DYNAMIC_FEATURE':
                return self._explain_dynamic_feature_signal(signal_type, feature_german, value, base_feature)
            elif role == 'STATIC_CATEGORICAL':
                return self._explain_static_feature_signal(signal_type, feature_german, value, base_feature)
            elif role == 'DYNAMIC_TIME_SERIES':
                return self._explain_timeseries_signal(signal_type, feature_german, value, base_feature)
            else:
                return self._generate_generic_signal_explanation(signal_type, feature_german, value)
                
        except Exception as e:
            # Fallback bei Fehlern
            return self._generate_generic_signal_explanation(signal_type, feature_german, value)
    
    def _explain_dynamic_feature_signal(self, signal_type: str, feature_german: str, value: float, base_feature: str) -> str:
        """Erkl√§rt Dynamic Feature Signale"""
        
        if signal_type == 'major_drop':
            return f"Drastischer R√ºckgang bei {feature_german} - Aktivit√§t ist um √ºber 50% eingebrochen"
        elif signal_type == 'decline':
            return f"Kontinuierlicher R√ºckgang bei {feature_german} - Nutzung nimmt stetig ab"
        elif signal_type == 'stop':
            return f"Kompletter Stopp bei {feature_german} - Kunde nutzt diesen Service nicht mehr"
        elif signal_type == 'volatility':
            return f"Unregelm√§√üige Nutzung von {feature_german} - Schwankende Kundenbindung"
        else:
            return f"Auff√§lliges Verhalten bei {feature_german} (Wert: {value:.2f})"
    
    def _explain_static_feature_signal(self, signal_type: str, feature_german: str, value: float, base_feature: str) -> str:
        """Erkl√§rt Static Feature Signale"""
        
        if signal_type == 'digital_drop':
            return f"Digitalisierungsrate stark gesunken - Kunde wird weniger digital"
        elif signal_type == 'digital_stagnation':
            return f"Digitalisierung stagniert - Keine Fortschritte bei digitalen Services"
        else:
            return f"Ver√§nderung bei {feature_german} erkannt (Wert: {value:.2f})"
    
    def _explain_timeseries_signal(self, signal_type: str, feature_german: str, value: float, base_feature: str) -> str:
        """Erkl√§rt Time Series Feature Signale"""
        
        if signal_type == 'major_drop':
            return f"Zeitreihen-Analyse zeigt: Drastischer Einbruch bei {feature_german}"
        elif signal_type == 'decline':
            return f"Zeitreihen-Trend: Kontinuierlicher R√ºckgang bei {feature_german}"
        elif signal_type == 'volatility':
            return f"Zeitreihen-Analyse: Hohe Schwankungen bei {feature_german}"
        else:
            return f"Zeitreihen-Anomalie bei {feature_german} (Wert: {value:.2f})"
    
    def _generate_generic_signal_explanation(self, signal_type: str, feature_german: str, value: float) -> str:
        """Fallback f√ºr generische Signal-Erkl√§rungen"""
        
        generic_explanations = {
            'major_drop': f"Starker R√ºckgang bei {feature_german} erkannt",
            'decline': f"R√ºckl√§ufiger Trend bei {feature_german}",
            'stop': f"Aktivit√§t bei {feature_german} gestoppt",
            'volatility': f"Schwankungen bei {feature_german}",
            'digital_drop': f"Digitale Aktivit√§t r√ºckl√§ufig",
            'digital_stagnation': f"Digitalisierung stagniert",
            'business_decline': f"Gesch√§ftsaktivit√§t nimmt ab",
            'composite_score': f"Multiple Risikofaktoren erkannt"
        }
        
        return generic_explanations.get(signal_type, f"Warnsignal bei {feature_german}: {value:.2f}")

    def _translate_feature_name(self, feature_name: str) -> str:
        """√úbersetzt technische Feature-Namen in gesch√§ftlich verst√§ndliche Begriffe - DATA-DICTIONARY-BASIERT"""
        
        # Pr√ºfe zuerst, ob Data Dictionary verf√ºgbar ist
        if not self.data_dictionary:
            return feature_name
            
        # Suche urspr√ºnglichen Spaltenname (entferne Feature-Engineering-Suffixe)
        base_feature = feature_name
        
        # Entferne typische Feature-Engineering-Suffixe
        suffixes_to_remove = [
            '_cum_mean', '_cum_std', '_cum_sum', '_cum_count',
            '_trend', '_ma_', '_volatility', '_risk', '_ew_', '_lag_',
            '_rolling_mean', '_rolling_std', '_shift', '_diff'
        ]
        
        for suffix in suffixes_to_remove:
            if suffix in base_feature:
                base_feature = base_feature.split(suffix)[0]
                break
        
        # Suche in Data Dictionary
        for column_name, column_info in self.data_dictionary['columns'].items():
            if column_name == base_feature:
                role = column_info.get('confirmed', {}).get('role', column_info.get('role', 'UNKNOWN'))
                data_type = column_info.get('confirmed', {}).get('data_type', column_info.get('data_type', 'TEXT'))
                
                # Generiere √úbersetzung basierend auf Rolle
                return self._generate_translation_by_role(base_feature, role, data_type)
        
        # Fallback: Versuche intelligente √úbersetzung basierend auf Namensmuster
        return self._generate_smart_translation(base_feature)
    
    def _generate_translation_by_role(self, feature_name: str, role: str, data_type: str) -> str:
        """Generiert √úbersetzung basierend auf Data Dictionary-Rolle"""
        
        # Basis-√úbersetzung
        clean_name = feature_name.replace('I_', '').replace('N_', '').replace('_', ' ').title()
        
        # Rollenbasierte √úbersetzungen
        if role == 'STATIC_CATEGORICAL':
            if 'digital' in feature_name.lower():
                return f"Digitalisierungsrate ({clean_name})"
            else:
                return f"Kategorisches Merkmal: {clean_name}"
                
        elif role == 'DYNAMIC_FEATURE':
            # Erkennung der Aktivit√§tsart
            if any(keyword in feature_name.lower() for keyword in ['maintenance', 'wartung']):
                return f"Wartungsservices ({clean_name})"
            elif any(keyword in feature_name.lower() for keyword in ['upgrade', 'upgrad']):
                return f"System-Upgrades ({clean_name})"
            elif any(keyword in feature_name.lower() for keyword in ['upsell', 'erweiterung']):
                return f"Produkt-Erweiterungen ({clean_name})"
            elif any(keyword in feature_name.lower() for keyword in ['downgrade', 'reduzierung']):
                return f"Produkt-Reduzierungen ({clean_name})"
            elif any(keyword in feature_name.lower() for keyword in ['downsell', 'service']):
                return f"Service-Reduzierungen ({clean_name})"
            elif any(keyword in feature_name.lower() for keyword in ['uhd', 'hd']):
                return f"UHD-Services ({clean_name})"
            elif any(keyword in feature_name.lower() for keyword in ['consulting', 'beratung']):
                return f"Beratungsleistungen ({clean_name})"
            elif any(keyword in feature_name.lower() for keyword in ['seminar', 'schulung']):
                return f"Schulungen/Seminare ({clean_name})"
            elif any(keyword in feature_name.lower() for keyword in ['insurance', 'versicherung']):
                return f"Versicherungsleistungen ({clean_name})"
            else:
                return f"Gesch√§ftsaktivit√§t: {clean_name}"
                
        elif role == 'DYNAMIC_TIME_SERIES':
            return f"Zeitreihen-Aktivit√§t: {clean_name}"
            
        elif role == 'STATIC_CONSTANT':
            return f"Konstantes Merkmal: {clean_name}"
            
        else:
            return f"Unbekannte Aktivit√§t: {clean_name}"
    
    def _generate_smart_translation(self, feature_name: str) -> str:
        """Fallback: Intelligente √úbersetzung basierend auf Namensmuster"""
        
        # Basis-Bereinigung
        clean_name = feature_name.replace('I_', '').replace('N_', '').replace('_', ' ').title()
        
        # Muster-basierte √úbersetzungen
        patterns = {
            'digital': 'Digitalisierung',
            'maintenance': 'Wartung',
            'upgrade': 'Upgrade',
            'upsell': 'Produkterweiterung',
            'downgrade': 'Produktreduzierung',
            'downsell': 'Service-Reduzierung',
            'consulting': 'Beratung',
            'seminar': 'Schulung',
            'insurance': 'Versicherung',
            'support': 'Support',
            'service': 'Service',
            'activity': 'Aktivit√§t',
            'rate': 'Rate',
            'score': 'Score',
            'count': 'Anzahl',
            'sum': 'Summe',
            'mean': 'Durchschnitt',
            'trend': 'Trend',
            'volatility': 'Schwankung'
        }
        
        feature_lower = feature_name.lower()
        for pattern, translation in patterns.items():
            if pattern in feature_lower:
                return f"{translation} ({clean_name})"
        
        # Fallback
        return clean_name

    def run_complete_analysis(self, force_retrain=False, training_from=None, training_to=None, test_from=None, test_to=None):
        """F√ºhrt komplette Enhanced Early Warning Analyse durch mit optimierten Konfigurationen"""
        
        # üîç SOFORTIGE PARAMETER-AUSGABE
        print("üîç" + "="*60 + "üîç")
        print("     PARAMETER-DEBUG")
        print("üîç" + "="*60 + "üîç")
        print(f"üéØ Training-Zeitr√§ume: {training_from} - {training_to}")
        print(f"üéØ Backtest-Zeitr√§ume: {test_from} - {test_to}")
        print(f"üîç Parameter-Typen:")
        print(f"   training_from: {type(training_from)} = {training_from}")
        print(f"   training_to: {type(training_to)} = {training_to}")
        print(f"   test_from: {type(test_from)} = {test_from}")
        print(f"   test_to: {type(test_to)} = {test_to}")
        print("üîç" + "="*60 + "üîç")
        
        # GLOBALE KONFIGURATION: Erzwinge echte Analysen
        global_config = get_global_config()
        enforce_real_analysis("Enhanced Early Warning Analysis")
        
        print("üö®" + "="*60 + "üö®")
        print("     ENHANCED EARLY WARNING SYSTEM")
        print("üö®" + "="*60 + "üö®")
        
        # üîß NEU: Training- und Backtest-Zeitr√§ume m√ºssen explizit angegeben werden
        if training_from is None or training_to is None:
            print("‚ùå FEHLER: Training-Zeitr√§ume m√ºssen explizit angegeben werden!")
            print("   training_from und training_to sind erforderlich")
            return {
                'error': 'Training-Zeitr√§ume nicht angegeben',
                'solution': '√úbergebe training_from und training_to Parameter'
            }
        
        if test_from is None or test_to is None:
            print("‚ùå FEHLER: Backtest-Zeitr√§ume m√ºssen explizit angegeben werden!")
            print("   test_from und test_to sind erforderlich")
            return {
                'error': 'Backtest-Zeitr√§ume nicht angegeben',
                'solution': '√úbergebe test_from und test_to Parameter'
            }
        
        print(f"üìÖ Training-Zeitr√§ume: {training_from} - {training_to}")
        print(f"üìÖ Backtest-Zeitr√§ume: {test_from} - {test_to}")
        print(f"üîç DEBUG: Parameter-Typen - training_from: {type(training_from)}, training_to: {type(training_to)}")
        print(f"üîç DEBUG: Parameter-Werte - training_from: {training_from}, training_to: {training_to}")
        
        # üîß NEU: Speichere Training- und Backtest-Zeitr√§ume f√ºr Modell-Speicherung
        self.current_training_periods = {
            'training_from': training_from,
            'training_to': training_to,
            'test_from': test_from,
            'test_to': test_to
        }
        
        try:
            # LADE OPTIMIERTE KONFIGURATION AUS FEATURE ANALYSIS ENGINE
            print("üîß Lade optimierte Konfiguration aus Feature Analysis Engine...")
            optimized_config = self._load_optimized_config()
            
            if not optimized_config:
                print("‚ùå KEINE OPTIMIERTE KONFIGURATION GEFUNDEN!")
                print("‚ö†Ô∏è  Bitte f√ºhre zuerst die Feature Analysis Engine aus!")
                return {
                    'error': 'Keine optimierte Konfiguration gefunden',
                    'solution': 'F√ºhre feature_analysis_engine.py aus'
                }
            
            print(f"‚úÖ Optimierte Konfiguration geladen:")
            print(f"   üéØ Modell: {optimized_config['model_type']}")
            if 'sampling_config' in optimized_config:
                print(f"   üéØ Sampling: {optimized_config['sampling_config']['strategy']}")
            else:
                print(f"   üéØ Sampling: Keine (Standard)")
            print(f"   üéØ AUC: 0.9938 (aus Feature Analysis Engine)")
            
            # INTELLIGENTE MODELL-VERWALTUNG
            if not force_retrain:
                # üîß NEU: Pr√ºfe Training-Zeitr√§ume beim Modell-Vergleich
                should_retrain, reason = self.should_retrain_model(training_from, training_to)
                
                if not should_retrain:
                    # Versuche existierendes Modell zu laden
                    if self.load_latest_model():
                        print(f"üéØ Verwende existierendes Modell - kein Training erforderlich ({reason})")
                        
                        # GLOBALE KONFIGURATION: Pr√ºfe ob Schnell-Modus erlaubt ist
                        # ABER: Respektiere force_retrain Parameter von UI
                        if global_config.is_simplified_mode_disabled() and not force_retrain:
                            print("üîÑ Vollst√§ndige Analyse erforderlich (globale Konfiguration)")
                            force_retrain = True
                        elif not force_retrain:
                            # Lade nur Basis-CSV f√ºr vereinfachtes Backtest (OHNE Feature-Engineering)
                            print("üîß Lade Basis-Daten √ºber DataAccessLayer f√ºr SCHNELL-MODUS...")
                            # MIGRATION: Verwende DataAccessLayer statt direkter CSV-Zugriff
                            self.df = self.dal.load_stage0_data()
                            # KRITISCH: Setze features_df NICHT - dadurch wird SCHNELL-MODUS aktiviert
                            self.features_df = None
                            
                            # Verwende existierendes Modell f√ºr Backtest
                            print("üéØ Verwende existierendes Modell f√ºr Backtest...")
                            # üîß ENTFERNT: Redundanter Backtest-Aufruf - verwende nur _run_full_analysis_with_optimized_config
                            return self._run_full_analysis_with_optimized_config(
                                optimized_config, 
                                test_from=test_from, test_to=test_to, 
                                use_existing_model=True,
                                training_from=training_from, training_to=training_to
                            )
                        else:
                            # Verwende existierendes Modell f√ºr vollst√§ndige Analyse
                            print("üéØ Verwende existierendes Modell f√ºr vollst√§ndige Analyse...")
                            return self._run_full_analysis_with_existing_model(optimized_config)
                    else:
                        print("‚ö†Ô∏è Kein existierendes Modell gefunden - Training erforderlich")
                        force_retrain = True
                else:
                    print(f"üîÑ Modell muss neu trainiert werden: {reason}")
                    force_retrain = True
            
            # VOLLST√ÑNDIGE ANALYSE MIT OPTIMIERTER KONFIGURATION
            if force_retrain:
                print("üîÑ F√ºhre vollst√§ndige Analyse mit optimierter Konfiguration durch...")
                return self._run_full_analysis_with_optimized_config(
                    optimized_config, 
                    test_from=test_from, test_to=test_to, 
                    use_existing_model=False,
                    training_from=training_from, training_to=training_to
                )
            
        except Exception as e:
            print(f"‚ùå Fehler in Enhanced Early Warning Analyse: {e}")
            import traceback
            traceback.print_exc()
            return {'error': str(e)}
    
    def _run_full_analysis_with_optimized_config(self, optimized_config, test_from=None, test_to=None, use_existing_model=False, training_from=None, training_to=None):
        """F√ºhrt vollst√§ndige Analyse mit optimierter Konfiguration durch"""
        print("üîß Lade und verarbeite Daten mit optimierter Konfiguration...")
        
        # Lade und verarbeite Features
        self.load_and_enhance_features()
        
        # üîß NEU: Verwende √ºbergebene Training-Zeitr√§ume
        if training_from is None or training_to is None:
            raise ValueError("‚ùå FEHLER: training_from und training_to m√ºssen explizit gesetzt werden! Keine Fallbacks erlaubt.")
        if test_from is None or test_to is None:
            raise ValueError("‚ùå FEHLER: test_from und test_to m√ºssen explizit gesetzt werden! Keine Fallbacks erlaubt.")
        prediction_timebase = test_from  # üîß DYNAMISCH basierend auf test_from
        
        print(f"üõ°Ô∏è DATA LEAKAGE PR√ÑVENTION: Training {training_from}-{training_to}, Prediction ab {prediction_timebase}")
        
        # Trainiere Modell mit optimierter Konfiguration
        print("ü§ñ Training mit optimierter Konfiguration...")
        
        # üíæ NEU: Speichere Enhanced Features in JSON-Database f√ºr CF-Pipeline
        print("üíæ Speichere Enhanced Features f√ºr CF-Pipeline...")
        if hasattr(self, 'experiment_id') and self.experiment_id:
            experiment_id = self.experiment_id
        else:
            experiment_id = 1  # Fallback f√ºr Experiment-ID
        
        save_success = self.save_enhanced_features_to_database(self.features_df, experiment_id)
        if save_success:
            print("‚úÖ Enhanced Features erfolgreich in JSON-Database gespeichert")
        else:
            print("‚ö†Ô∏è Enhanced Features konnten nicht gespeichert werden")

        # Real-World Backtest
        print("üìä Real-World Backtest mit optimierter Konfiguration...")
        backtest_results = self.real_world_backtest(
            training_from=training_from,
            training_to=training_to,
            test_from=test_from,
            test_to=test_to,
            use_existing_model=use_existing_model
        )
        
        if backtest_results and 'error' not in backtest_results:
            print("‚úÖ Vollst√§ndige Analyse erfolgreich abgeschlossen")
            
            # Extract training metrics from backtest results
            training_metrics = {
                'precision': backtest_results.get('precision', 0.0),
                'recall': backtest_results.get('recall', 0.0),
                'f1_score': backtest_results.get('f1_score', 0.0),
                'auc': backtest_results.get('auc', 0.0),
                'accuracy': backtest_results.get('accuracy', 0.0)
            }
            
            return {
                'status': 'success',
                'mode': 'full_analysis',
                'training_metrics': training_metrics,
                'backtest_results': backtest_results,
                'optimized_config': optimized_config,
                'note': 'Verwendet optimierte Konfiguration aus Feature Analysis Engine'
            }
        else:
            error_msg = backtest_results.get('error', 'Unbekannter Fehler') if backtest_results else 'Keine Backtest-Ergebnisse'
            print(f"‚ùå Vollst√§ndige Analyse fehlgeschlagen: {error_msg}")
            return {'error': error_msg}
    
    def _run_full_analysis_with_existing_model(self, optimized_config):
        """F√ºhrt vollst√§ndige Analyse mit existierendem Modell durch"""
        print("üîß Lade und verarbeite Daten...")
        
        # Lade und verarbeite Features
        self.load_and_enhance_features()
        
        # üîß ENTFERNT: Redundanter Backtest-Aufruf - verwende nur _run_full_analysis_with_optimized_config
        print("üéØ Verwende existierendes Modell f√ºr vollst√§ndige Analyse...")
        return self._run_full_analysis_with_optimized_config(
            optimized_config, 
            test_from=None, test_to=None,  # üîß DYNAMISCH - Default-Werte werden in der Funktion gesetzt
            use_existing_model=True
        )

    def save_enhanced_model(self, model, metrics: dict, feature_names: list, training_info: dict = None):
        """Speichert das Enhanced Early Warning Modell"""
        try:
            
            # Models-Verzeichnis
            models_dir = ProjectPaths.get_models_directory()
            models_dir.mkdir(exist_ok=True)
            
            # Timestamp f√ºr eindeutige Dateinamen
            timestamp = datetime.now().strftime("%Y-%m-%dT%H-%M-%S")
            
            # Modell-Dateiname
            model_name = f"Enhanced_EarlyWarning_{timestamp}.joblib"
            model_path = models_dir / model_name
            
            # Modell speichern
            joblib.dump(model, model_path)
            print(f"üíæ Enhanced Early Warning Modell gespeichert: {model_path}")
            
            # CSV-Metadaten f√ºr intelligente Modell-Verwaltung
            csv_metadata = self.get_csv_metadata()
            
            # üîß NEU: Training-Zeitr√§ume aus training_info oder current_training_periods extrahieren
            training_periods = {}
            if training_info:
                if 'training_from' in training_info and 'training_to' in training_info:
                    training_periods = {
                        'training_from': training_info['training_from'],
                        'training_to': training_info['training_to']
                    }
                elif 'training_period' in training_info:
                    # Fallback: Aus training_period String extrahieren
                    period_str = training_info['training_period']
                    if '-' in period_str:
                        parts = period_str.split('-')
                        if len(parts) == 2:
                            training_periods = {
                                'training_from': parts[0].strip(),
                                'training_to': parts[1].strip()
                            }
            elif hasattr(self, 'current_training_periods'):
                # üîß NEU: Verwende current_training_periods aus run_complete_analysis
                training_periods = self.current_training_periods
            
            # Metadaten erstellen
            metadata = {
                "created": timestamp,
                "model_type": "Enhanced_EarlyWarning",
                "training_approach": "early_warning_backtest",
                "feature_names": feature_names,
                "metrics": metrics,
                "csv_metadata": csv_metadata,  # NEU: CSV-Metadaten f√ºr intelligente Pr√ºfung
                "training_periods": training_periods,  # üîß NEU: Training-Zeitr√§ume
                "training_info": training_info or {
                    "algorithm": "RandomForest",
                    "level": "enhanced_early_warning",
                    "features_count": len(feature_names),
                    "timestamp": timestamp,
                    "compatibility": "system_health_ui",
                    "lead_time_months": 6,
                    "warning_signals": len([f for f in feature_names if '_ew_' in f])
                }
            }
            
            # Metadaten speichern
            metadata_name = f"Enhanced_EarlyWarning_{timestamp}.json"
            metadata_path = models_dir / metadata_name
            
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            print(f"üìÑ Enhanced Early Warning Metadaten gespeichert: {metadata_path}")
            
            if csv_metadata:
                print(f"üìä CSV-Metadaten gespeichert: {csv_metadata['file_name']} ({csv_metadata['file_size']} bytes)")
            
            if training_periods:
                print(f"üìÖ Training-Zeitr√§ume gespeichert: {training_periods['training_from']} - {training_periods['training_to']}")
            
            return str(model_path), str(metadata_path)
            
        except Exception as e:
            print(f"‚ùå Fehler beim Speichern des Enhanced Early Warning Modells: {e}")
            return None, None

    def _save_backtest_results(self, test_period_df, validation_customers, backtest_results):
        """Speichert Backtest-Ergebnisse mit allen Customer Predictions"""
        try:
            from datetime import datetime
            import json
            
            # Models-Verzeichnis
            models_dir = ProjectPaths.get_models_directory()
            models_dir.mkdir(exist_ok=True)
            
            # Timestamp f√ºr eindeutige Dateinamen
            timestamp = datetime.now().strftime("%Y-%m-%dT%H-%M-%S")
            
            # Backtest-Ergebnisse anreichern (Z√§hler und Segmente)
            try:
                total_samples = int(len(test_period_df)) if test_period_df is not None else 0
            except Exception:
                total_samples = 0

            high_count = 0
            low_count = 0
            try:
                if test_period_df is not None and 'RISK_LEVEL' in test_period_df.columns:
                    high_count = int((test_period_df['RISK_LEVEL'] == 'HIGH').sum())
                    low_count = int((test_period_df['RISK_LEVEL'] == 'LOW').sum())
            except Exception:
                pass

            # Erg√§nze Z√§hler in backtest_results
            if isinstance(backtest_results, dict):
                backtest_results = dict(backtest_results)
                backtest_results.setdefault('backtest_samples', total_samples)
                backtest_results.setdefault('high_risk_customers', high_count)
                backtest_results.setdefault('medium_risk_customers', 0)
                backtest_results.setdefault('low_risk_customers', low_count)

            # Backtest-Ergebnisse erstellen
            backtest_data = {
                "created": timestamp,
                "model_type": "Enhanced_EarlyWarning_Backtest",
                "backtest_results": backtest_results,
                "test_period_customers_count": total_samples,
                "test_period_customers_data": test_period_df.to_dict('records') if test_period_df is not None and len(test_period_df) > 0 else [],
                "validation_customers_count": len(validation_customers) if validation_customers is not None else 0,
                "validation_customers": validation_customers if validation_customers is not None else [],
                "feature_names": self.feature_names if hasattr(self, 'feature_names') else [],
                "optimal_threshold": backtest_results.get('optimal_threshold', 0.5),
                "backtest_period": backtest_results.get('backtest_period', {})
            }
            
            # Backtest-Ergebnisse speichern
            backtest_name = f"Enhanced_EarlyWarning_Backtest_{timestamp}.json"
            backtest_path = models_dir / backtest_name
            
            with open(backtest_path, 'w', encoding='utf-8') as f:
                json.dump(backtest_data, f, indent=2, ensure_ascii=False)
            
            print(f"üìä Backtest-Ergebnisse gespeichert: {backtest_path}")
            print(f"   - Test-Zeitraum-Kunden: {len(test_period_df) if test_period_df is not None else 0}")
            print(f"   - Validation Customers: {len(validation_customers) if validation_customers is not None else 0}")
            print(f"   - Optimal Threshold: {backtest_results.get('optimal_threshold', 0.5)}")
            print(f"   - Backtest AUC: {backtest_results.get('auc', 'N/A')}")
            
            return str(backtest_path)
            
        except Exception as e:
            print(f"‚ùå Fehler beim Speichern der Backtest-Ergebnisse: {e}")
            return None

    def find_optimal_global_windows(self, max_windows=5):
        """Finde heuristische Lookback-Monate basierend auf Business-Zyklen"""
        print("üîç Verwende heuristische Business-Zyklen Windows...")
        
        # Heuristische Windows basierend auf typischen Business-Zyklen
        # 6M: Halbj√§hrliche Reviews, 12M: Jahresvertr√§ge, 18M: 1.5-Jahres-Zyklen
        # 24M: 2-Jahres-Vertr√§ge, 36M: Langfristige Enterprise-Vertr√§ge
        heuristic_windows = [6, 12, 18, 24, 36]
        
        print(f"‚úÖ Heuristische Business-Zyklen Windows: {heuristic_windows}")
        print("üìä Begr√ºndung:")
        print("   - 6M:  Halbj√§hrliche Reviews & Preisanpassungen")
        print("   - 12M: Jahresvertr√§ge & Budget-Zyklen") 
        print("   - 18M: 1.5-Jahres-Technologie-Zyklen")
        print("   - 24M: 2-Jahres-Enterprise-Vertr√§ge")
        print("   - 36M: Langfristige Strategic-Partnerships")
        
        return heuristic_windows

    def _calculate_gmm_risk_segments(self, y_pred_proba, optimal_threshold, dataset_name):
        """
        GMM-basierte Risiko-Segmentierung
        
        Args:
            y_pred_proba: Churn-Wahrscheinlichkeiten
            optimal_threshold: Optimaler Schwellwert
            dataset_name: Name des Datensatzes (Training/Test)
            
        Returns:
            risk_levels: Liste der Risiko-Level
            cluster_stats: Cluster-Statistiken
        """
        try:
            from sklearn.mixture import GaussianMixture
            import numpy as np
            
            print(f"üî¨ GMM-Clustering f√ºr {dataset_name}-Daten...")
            
            # Reshape f√ºr GMM
            X = y_pred_proba.reshape(-1, 1)
            
            # BIC-Optimierung f√ºr optimale Anzahl Komponenten
            n_components_range = range(2, 6)  # 2-5 Cluster
            bic = []
            aic = []
            
            for n in n_components_range:
                gmm = GaussianMixture(n_components=n, random_state=42, covariance_type='full')
                gmm.fit(X)
                bic.append(gmm.bic(X))
                aic.append(gmm.aic(X))
            
            # W√§hle optimale Anzahl basierend auf BIC
            optimal_n = n_components_range[np.argmin(bic)]
            print(f"   Optimal Cluster-Anzahl (BIC): {optimal_n}")
            
            # Fitte finales GMM
            best_gmm = GaussianMixture(n_components=optimal_n, random_state=42, covariance_type='full')
            clusters = best_gmm.fit_predict(X)
            
            # Cluster nach Risiko sortieren (niedrig ‚Üí hoch)
            cluster_means = best_gmm.means_.flatten()
            cluster_order = np.argsort(cluster_means)
            
            # Risiko-Level zuordnen
            risk_levels_map = ['VERY_LOW', 'LOW', 'MEDIUM', 'HIGH', 'VERY_HIGH']
            risk_mapping = dict(zip(cluster_order, risk_levels_map[:len(cluster_order)]))
            
            # Risiko-Level zuweisen
            risk_levels = [risk_mapping[cluster] for cluster in clusters]
            
            # Cluster-Statistiken berechnen
            cluster_stats = {
                'risk_levels': list(set(risk_levels)),
                'cluster_means': {risk_mapping[i]: float(cluster_means[i]) for i in range(len(cluster_means))},
                'cluster_sizes': {},
                'bic_scores': [float(score) for score in bic],
                'aic_scores': [float(score) for score in aic],
                'optimal_n_components': optimal_n
            }
            
            # Cluster-Gr√∂√üen
            for risk_level in cluster_stats['risk_levels']:
                cluster_stats['cluster_sizes'][risk_level] = risk_levels.count(risk_level)
            
            print(f"   Cluster gefunden: {cluster_stats['risk_levels']}")
            print(f"   Cluster-Gr√∂√üen: {cluster_stats['cluster_sizes']}")
            print(f"   Cluster-Mittelwerte: {cluster_stats['cluster_means']}")
            
            return risk_levels, cluster_stats
            
        except Exception as e:
            print(f"‚ùå Fehler bei GMM-Clustering: {e}")
            # Fallback: Bin√§re Segmentierung
            risk_levels = ['LOW' if prob < optimal_threshold else 'HIGH' for prob in y_pred_proba]
            cluster_stats = {
                'risk_levels': ['LOW', 'HIGH'],
                'cluster_means': {'LOW': 0.3, 'HIGH': 0.7},
                'cluster_sizes': {'LOW': risk_levels.count('LOW'), 'HIGH': risk_levels.count('HIGH')},
                'fallback': True
            }
            return risk_levels, cluster_stats

    def _compare_train_test_clusters(self, train_cluster_stats, test_cluster_stats):
        """
        Vergleicht Training- und Test-Cluster
        
        Args:
            train_cluster_stats: Training-Cluster-Statistiken
            test_cluster_stats: Test-Cluster-Statistiken
            
        Returns:
            Vergleichs-Statistiken
        """
        try:
            # Cluster-Anzahl-Vergleich
            train_n = len(train_cluster_stats['risk_levels'])
            test_n = len(test_cluster_stats['risk_levels'])
            
            # Stabilit√§ts-Score (0-1, 1 = perfekt stabil)
            stability_score = 1.0 if train_n == test_n else 0.5
            
            # Cluster-Mittelwert-Vergleich
            cluster_comparison = {}
            for risk_level in set(train_cluster_stats['risk_levels']) | set(test_cluster_stats['risk_levels']):
                train_mean = train_cluster_stats['cluster_means'].get(risk_level, 0)
                test_mean = test_cluster_stats['cluster_means'].get(risk_level, 0)
                cluster_comparison[risk_level] = {
                    'train_mean': train_mean,
                    'test_mean': test_mean,
                    'difference': abs(train_mean - test_mean)
                }
            
            comparison_stats = {
                'stability_score': stability_score,
                'train_clusters': train_n,
                'test_clusters': test_n,
                'cluster_comparison': cluster_comparison,
                'stable': stability_score > 0.8
            }
            
            return comparison_stats
            
        except Exception as e:
            print(f"‚ùå Fehler beim Cluster-Vergleich: {e}")
            return {
                'stability_score': 0.0,
                'train_clusters': 0,
                'test_clusters': 0,
                'cluster_comparison': {},
                'stable': False
            }

    def save_enhanced_features_to_database(self, features_df, experiment_id):
        """
        Speichert Enhanced Features in customer_churn_details Tabelle der JSON-Database
        
        Args:
            features_df: DataFrame mit Enhanced Features
            experiment_id: Experiment-ID
            
        Returns:
            bool: True wenn erfolgreich
        """
        try:
            print(f"üíæ Speichere Enhanced Features in JSON-Database f√ºr Experiment {experiment_id}...")
            
            # Import JSON-Database
            from bl.json_database.churn_json_database import ChurnJSONDatabase
            from datetime import datetime
            
            # Initialisiere Database
            db = ChurnJSONDatabase()
            
            # Dynamische Spaltennamen aus Data Dictionary
            primary_key_col, timebase_col, target_col = self._get_dynamic_column_names()
            
            # Erstelle Customer-Level Dataset mit Enhanced Features
            customer_df = self.create_customer_dataset(prediction_timebase=202401)  # Verwende aktuellen Zeitraum
            
            # Bereite Records f√ºr customer_churn_details vor
            enhanced_records = []
            
            for _, row in customer_df.iterrows():
                # Basis-Record mit bestehender Struktur
                record = {
                    'Kunde': int(row[primary_key_col]),
                    'experiment_id': experiment_id,
                    'Letzte_Timebase': int(row.get(timebase_col, 202401)),
                    'I_ALIVE': int(row.get(target_col, 1)),
                    'source': 'enhanced_early_warning',
                    'dt_created': datetime.now().isoformat()
                }
                
                # F√ºge ALLE Enhanced Features hinzu
                for feature_name in self.feature_names:
                    if feature_name in row.index:
                        feature_value = row[feature_name]
                        # Konvertiere zu Python-Standard-Typen
                        if pd.isna(feature_value):
                            record[feature_name] = None
                        elif isinstance(feature_value, (int, float)):
                            record[feature_name] = float(feature_value)
                        else:
                            record[feature_name] = feature_value
                
                enhanced_records.append(record)
            
            # L√∂sche existierende Records f√ºr dieses Experiment (falls vorhanden)
            if "customer_churn_details" in db.data["tables"]:
                existing_records = db.data["tables"]["customer_churn_details"]["records"]
                # Filtere Records f√ºr andere Experimente
                filtered_records = [r for r in existing_records if r.get('experiment_id') != experiment_id]
                db.data["tables"]["customer_churn_details"]["records"] = filtered_records
                print(f"üóëÔ∏è Alte Records f√ºr Experiment {experiment_id} entfernt")
            
            # F√ºge neue Enhanced Records hinzu
            success = db.add_customer_churn_details(enhanced_records)
            
            if success:
                # Speichere Database
                db.save()
                print(f"‚úÖ {len(enhanced_records)} Enhanced Customer Records in JSON-Database gespeichert")
                print(f"üìä Features pro Customer: {len(self.feature_names)}")
                print(f"üéØ Enhanced Features verf√ºgbar f√ºr CF-Pipeline")
                return True
            else:
                print("‚ùå Fehler beim Hinzuf√ºgen der Enhanced Records")
                return False
                
        except Exception as e:
            print(f"‚ùå Fehler beim Speichern der Enhanced Features: {e}")
            import traceback
            traceback.print_exc()
            return False

def get_dynamic_oversampling_config():
    """
    Liefert dynamische Oversampling-Konfiguration basierend auf aktuellen Daten
    
    Returns:
        dict: Oversampling-Konfiguration
    """
    try:
        # Lade Data Dictionary f√ºr Oversampling-Faktor
        paths = ProjectPaths()
        data_dict_path = paths.data_dictionary_file()
        
        if data_dict_path.exists():
            with open(data_dict_path, 'r', encoding='utf-8') as f:
                data_dict = json.load(f)
            
            # Oversampling-Faktor aus Data Dictionary
            oversampling_factor = data_dict.get('oversampling_factor', 5)
        else:
            # Fallback-Wert
            oversampling_factor = 5
        
        return {
            'oversampling_factor': oversampling_factor,
            'method': 'SMOTE',
            'random_state': 42,
            'sampling_strategy': 0.3,  # 30% der Minderheitsklasse
            'k_neighbors': 5,
            'source': 'data_dictionary.json' if data_dict_path.exists() else 'default'
        }
        
    except Exception as e:
        print(f"‚ö†Ô∏è Fehler beim Laden der Oversampling-Konfiguration: {e}")
        # Fallback-Konfiguration
        return {
            'oversampling_factor': 5,
            'method': 'SMOTE',
            'random_state': 42,
            'sampling_strategy': 0.3,
            'k_neighbors': 5,
            'source': 'fallback'
        }

def main():
    """Hauptfunktion"""
    system = EnhancedEarlyWarningSystem()
    results = system.run_complete_analysis()
    return system, results

if __name__ == "__main__":
    enhanced_system, results = main() 