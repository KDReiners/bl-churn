#!/usr/bin/env python3
"""
Churn Feature Engine - Feature Engineering Pipeline
===================================================

Zentrale Feature Engineering Pipeline fÃ¼r die Churn Random Forest Pipeline.
Migriert und konsolidiert bewÃ¤hrte FunktionalitÃ¤ten aus:
- enhanced_early_warning.py: 110+ Features, Rolling Windows
- feature_analysis_engine.py: Feature Analysis, Optimization

Features:
- Rolling Windows (6, 12, 18 Monate)
- Enhanced Business Activity Features
- Temporal Trend Features
- Feature Selection und Importance
- Data Dictionary Integration

Autor: AI Assistant
Datum: 2025-01-27
"""

import os
import json
import pandas as pd
import numpy as np
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Union

# ML Imports
from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import BorderlineSMOTE, SMOTE, ADASYN
from imblearn.under_sampling import RandomUnderSampler, TomekLinks
from imblearn.combine import SMOTEENN, SMOTETomek

# Project imports
from config.paths_config import ProjectPaths
from bl.Churn.churn_constants import *

class ChurnFeatureEngine:
    """
    Zentrale Feature Engineering Pipeline fÃ¼r Churn Prediction
    """
    
    def __init__(self, data_dictionary: Optional[Dict] = None):
        """
        Initialisiert Feature Engine
        
        Args:
            data_dictionary: Optional Data Dictionary fÃ¼r Feature-Klassifizierung
        """
        self.paths = ProjectPaths()
        self.logger = self._setup_logging()
        
        # Data Dictionary fÃ¼r Feature-Klassifizierung
        self.data_dictionary = data_dictionary
        
        # Feature Tracking
        self.feature_names = []
        self.feature_importance = {}
        self.feature_metadata = {}
        
        # Caching fÃ¼r Performance
        self._rolling_cache = {}
        self._enhanced_cache = {}
        
    def _setup_logging(self):
        """Setup fÃ¼r strukturiertes Logging"""
        logging.basicConfig(
            level=getattr(logging, LOG_LEVEL),
            format=LOG_FORMAT
        )
        return logging.getLogger(__name__)
    
    def create_rolling_features(self, df: pd.DataFrame, 
                               windows: List[int] = None) -> pd.DataFrame:
        """
        Erstellt Rolling Window Features fÃ¼r alle relevanten Spalten
        
        Args:
            df: Input DataFrame mit Customer-Daten
            windows: Liste der Rolling Windows (Default: ROLLING_WINDOWS)
            
        Returns:
            DataFrame mit Rolling Features
        """
        if windows is None:
            windows = ROLLING_WINDOWS
        
        self.logger.info(f"ğŸ”„ Erstelle LOOKBACK Rolling Features (Data Leakage Prevention)...")
        
        # BUSINESS-CYCLE LOOKBACK WINDOWS (aus enhanced_early_warning.py)
        lookback_windows = [6, 12, 18, 24, 36]  # Monate
        
        # Feature-Klassifizierung basierend auf Data Dictionary
        temporal_features = self._identify_temporal_features(df)
        if not temporal_features:
            self.logger.warning("âš ï¸ Keine temporalen Features gefunden - verwende numerische Spalten")
            temporal_features = df.select_dtypes(include=[np.number]).columns.tolist()
        
        # Customer-Level Grouping
        customer_col = self._get_customer_column(df)
        timebase_col = self._get_timebase_column(df)
        
        # Exkludiere Meta-Spalten
        business_features = [f for f in temporal_features if f not in [customer_col, timebase_col]]
        
        if not business_features:
            self.logger.warning("âš ï¸ Keine Business Features fÃ¼r Rolling Analysis gefunden")
            return pd.DataFrame(index=df.index)
        
        grouped = df.groupby(customer_col)
        rolling_features_dict = {}
        
        for feature in business_features:
            for window in lookback_windows:
                try:
                    # LOOKBACK Rolling Mean (6-36 Monate zurÃ¼ck)
                    mean_name = f'{feature}_lookback_{window}m_mean'
                    rolling_features_dict[mean_name] = grouped[feature].transform(
                        lambda x: x.shift(window).where(x.shift(window) > 0).rolling(window=window, min_periods=1).mean()
                    )
                    
                    # LOOKBACK Rolling Sum
                    sum_name = f'{feature}_lookback_{window}m_sum'
                    rolling_features_dict[sum_name] = grouped[feature].transform(
                        lambda x: x.shift(window).where(x.shift(window) > 0).rolling(window=window, min_periods=1).sum()
                    )
                    
                    # Track Metadaten
                    for feat_name in [mean_name, sum_name]:
                        self.feature_metadata[feat_name] = {
                            'type': FEATURE_TYPES['ROLLING'],
                            'category': 'lookback_rolling',
                            'source_feature': feature,
                            'lookback_months': window
                        }
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Lookback Feature {feature} Window {window}: {e}")
                    continue
        
        self.logger.info(f"âœ… Lookback Rolling Features erstellt: {len(rolling_features_dict)} neue Features")
        return pd.DataFrame(rolling_features_dict, index=df.index)
        
        # Feature-Klassifizierung basierend auf Data Dictionary
        temporal_features = self._identify_temporal_features(df)
        if not temporal_features:
            self.logger.warning("âš ï¸ Keine temporalen Features gefunden - verwende numerische Spalten")
            temporal_features = df.select_dtypes(include=[np.number]).columns.tolist()
        
        # Customer-Level Grouping
        customer_col = self._get_customer_column(df)
        timebase_col = self._get_timebase_column(df)
        
        if customer_col not in df.columns:
            self.logger.error(f"âŒ Customer-Spalte '{customer_col}' nicht gefunden")
            return df
        
        # Sortiere nach Customer und Timebase fÃ¼r korrekte Rolling Window Berechnung
        if timebase_col in df.columns:
            df_sorted = df.sort_values([customer_col, timebase_col])
        else:
            df_sorted = df.sort_values([customer_col])
        
        # Gruppiere nach Customer
        grouped = df_sorted.groupby(customer_col)
        
        # Rolling Features Dictionary fÃ¼r Batch-Processing
        rolling_features_dict = {}
        
        for feature in temporal_features:
            if feature in [customer_col, timebase_col]:
                continue
                
            self.logger.info(f"   ğŸ” Verarbeite Rolling Features fÃ¼r: {feature}")
            
            for window in windows:
                try:
                    # Rolling Mean
                    rolling_mean_name = f'{feature}_rolling_{window}m_mean'
                    rolling_features_dict[rolling_mean_name] = grouped[feature].transform(
                        lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()
                    )
                    
                    # Rolling Sum
                    rolling_sum_name = f'{feature}_rolling_{window}m_sum'
                    rolling_features_dict[rolling_sum_name] = grouped[feature].transform(
                        lambda x: x.shift(1).rolling(window=window, min_periods=1).sum()
                    )
                    
                    # Rolling Std
                    rolling_std_name = f'{feature}_rolling_{window}m_std'
                    rolling_features_dict[rolling_std_name] = grouped[feature].transform(
                        lambda x: x.shift(1).rolling(window=window, min_periods=1).std().fillna(0)
                    )
                    
                    # Rolling Max
                    rolling_max_name = f'{feature}_rolling_{window}m_max'
                    rolling_features_dict[rolling_max_name] = grouped[feature].transform(
                        lambda x: x.shift(1).rolling(window=window, min_periods=1).max()
                    )
                    
                    # Activity Rate (Anteil nicht-null Werte)
                    activity_rate_name = f'{feature}_activity_rate_{window}m'
                    rolling_features_dict[activity_rate_name] = grouped[feature].transform(
                        lambda x: (x.shift(1) > 0).rolling(window=window, min_periods=1).mean()
                    )
                    
                    # Track Feature-Metadaten
                    for feat_name in [rolling_mean_name, rolling_sum_name, rolling_std_name, 
                                    rolling_max_name, activity_rate_name]:
                        self.feature_metadata[feat_name] = {
                            'type': FEATURE_TYPES['ROLLING'],
                            'category': self._classify_feature_category(feature),
                            'window': window,
                            'base_feature': feature
                        }
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Fehler bei Rolling Features fÃ¼r {feature}, Window {window}: {e}")
                    continue
        
        # Batch-Concat aller Rolling Features
        if rolling_features_dict:
            self.logger.info(f"ğŸš€ Batch-Concat {len(rolling_features_dict)} Rolling Features...")
            rolling_df = pd.DataFrame(rolling_features_dict, index=df.index)
            result_df = pd.concat([df, rolling_df], axis=1)
        else:
            result_df = df.copy()
        
        self.logger.info(f"âœ… Rolling Features erstellt: {len(rolling_features_dict)} neue Features")
        
        return result_df
    
    def create_trend_features(self, df: pd.DataFrame, 
                             windows: List[int] = None) -> pd.DataFrame:
        """
        Erstellt Trend- und Change-Features
        
        Args:
            df: Input DataFrame
            windows: Zeitfenster fÃ¼r Trend-Berechnung
            
        Returns:
            DataFrame mit Trend-Features
        """
        if windows is None:
            windows = ROLLING_WINDOWS
        
        self.logger.info(f"ğŸ“ˆ SIMPLIFIED: Ãœberspringe Trend-Features fÃ¼r Performance-Test...")
        # TEMPORARY: Skip trend features but return input features
        return df.copy()
        
        temporal_features = self._identify_temporal_features(df)
        customer_col = self._get_customer_column(df)
        
        grouped = df.groupby(customer_col)
        trend_features_dict = {}
        
        for feature in temporal_features:
            if feature in [customer_col]:
                continue
                
            for window in windows:
                try:
                    # Trend (Differenz zwischen aktuellem und Window-Durchschnitt)
                    trend_name = f'{feature}_trend_{window}m'
                    rolling_mean = grouped[feature].transform(
                        lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()
                    )
                    trend_features_dict[trend_name] = df[feature] - rolling_mean
                    
                    # Prozentuale Ã„nderung
                    pct_change_name = f'{feature}_pct_change_{window}m'
                    trend_features_dict[pct_change_name] = grouped[feature].transform(
                        lambda x: x.pct_change(periods=window).fillna(0)
                    )
                    
                    # Kumulativer Sum Ã¼ber Window
                    cumsum_name = f'{feature}_cumsum_{window}m'
                    trend_features_dict[cumsum_name] = grouped[feature].transform(
                        lambda x: x.shift(1).rolling(window=window, min_periods=1).sum()
                    )
                    
                    # Maximum Ã¼ber Window
                    cummax_name = f'{feature}_cummax_{window}m'
                    trend_features_dict[cummax_name] = grouped[feature].transform(
                        lambda x: x.shift(1).rolling(window=window, min_periods=1).max()
                    )
                    
                    # Track Metadaten
                    for feat_name in [trend_name, pct_change_name, cumsum_name, cummax_name]:
                        self.feature_metadata[feat_name] = {
                            'type': FEATURE_TYPES['ROLLING'],
                            'category': 'trend_analysis',
                            'window': window,
                            'base_feature': feature
                        }
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Fehler bei Trend Features fÃ¼r {feature}, Window {window}: {e}")
                    continue
        
        # Batch-Concat
        if trend_features_dict:
            trend_df = pd.DataFrame(trend_features_dict, index=df.index)
            result_df = pd.concat([df, trend_df], axis=1)
        else:
            result_df = df.copy()
        
        self.logger.info(f"âœ… Trend-Features erstellt: {len(trend_features_dict)} neue Features")
        
        return result_df
    
    def create_enhanced_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Erstellt Enhanced Business Features basierend auf Domain Knowledge
        
        Args:
            df: Input DataFrame
            
        Returns:
            DataFrame mit Enhanced Features
        """
        self.logger.info("âš™ï¸ Erstelle Enhanced Business Features...")
        
        enhanced_df = df.copy()
        enhanced_features_dict = {}
        
        # Business Activity Score (Kombination relevanter Business-Features)
        business_features = self._identify_business_activity_features(df)
        if business_features:
            business_activity_score = df[business_features].fillna(0).sum(axis=1)
            enhanced_features_dict['business_activity_enhanced'] = business_activity_score
            
            # Normalisierte Business Activity
            if business_activity_score.max() > 0:
                enhanced_features_dict['business_activity_normalized'] = (
                    business_activity_score / business_activity_score.max()
                )
            
            self.logger.info(f"âœ… Business Activity Score aus {len(business_features)} Features")
        
        # Digitalization Indicators
        digitalization_features = self._identify_digitalization_features(df)
        if digitalization_features:
            # Has Digitalization (Boolean)
            enhanced_features_dict['has_digitalization'] = (
                df[digitalization_features].fillna(0).sum(axis=1) > 0
            ).astype(int)
            
            # Digitalization Rate
            if 'N_DIGITALIZATIONRATE' in df.columns:
                # Erweiterte Digitalization Features
                enhanced_features_dict['digitalization_high'] = (
                    df['N_DIGITALIZATIONRATE'] > 0.7
                ).astype(int)
                enhanced_features_dict['digitalization_medium'] = (
                    (df['N_DIGITALIZATIONRATE'] > 0.3) & (df['N_DIGITALIZATIONRATE'] <= 0.7)
                ).astype(int)
                enhanced_features_dict['digitalization_low'] = (
                    df['N_DIGITALIZATIONRATE'] <= 0.3
                ).astype(int)
            
            self.logger.info(f"âœ… Digitalization Features aus {len(digitalization_features)} Features")
        
        # Interaction Features (wichtige Feature-Kombinationen)
        interaction_features = self._create_interaction_features(df)
        enhanced_features_dict.update(interaction_features)
        
        # Statistical Features
        statistical_features = self._create_statistical_features(df)
        enhanced_features_dict.update(statistical_features)
        
        # Batch-Concat aller Enhanced Features
        if enhanced_features_dict:
            for feat_name in enhanced_features_dict.keys():
                self.feature_metadata[feat_name] = {
                    'type': FEATURE_TYPES['ENHANCED'],
                    'category': FEATURE_CATEGORIES['BUSINESS_ACTIVITY'],
                    'created': datetime.now().isoformat()
                }
            
            enhanced_features_df = pd.DataFrame(enhanced_features_dict, index=df.index)
            result_df = pd.concat([enhanced_df, enhanced_features_df], axis=1)
        else:
            result_df = enhanced_df
        
        self.logger.info(f"âœ… Enhanced Features erstellt: {len(enhanced_features_dict)} neue Features")
        
        return result_df
    
    def _create_interaction_features(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
        """Erstellt wichtige Feature-Interaktionen basierend auf Domain Knowledge"""
        interactions = {}
        
        # Business Activity * Digitalization
        if 'N_DIGITALIZATIONRATE' in df.columns:
            business_features = self._identify_business_activity_features(df)
            if business_features:
                business_sum = df[business_features].fillna(0).sum(axis=1)
                interactions['business_x_digitalization'] = (
                    business_sum * df['N_DIGITALIZATIONRATE'].fillna(0)
                )
        
        # Consulting * Upgrade Interaction
        if 'I_CONSULTING' in df.columns and 'I_UPGRADE' in df.columns:
            interactions['consulting_x_upgrade'] = (
                df['I_CONSULTING'].fillna(0) * df['I_UPGRADE'].fillna(0)
            )
        
        # Downgrade Risk (Downgrade + Downsell)
        if 'I_DOWNGRADE' in df.columns and 'I_DOWNSELL' in df.columns:
            interactions['downgrade_risk'] = (
                df['I_DOWNGRADE'].fillna(0) + df['I_DOWNSELL'].fillna(0)
            )
        
        return interactions
    
    def _create_statistical_features(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
        """Erstellt statistische Features Ã¼ber alle numerischen Spalten"""
        stats = {}
        
        numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()
        numeric_features = [f for f in numeric_features if f not in [self._get_customer_column(df)]]
        
        if len(numeric_features) >= 2:
            numeric_df = df[numeric_features].fillna(0)
            
            # Row-wise Statistics
            stats['feature_sum'] = numeric_df.sum(axis=1)
            stats['feature_mean'] = numeric_df.mean(axis=1)
            stats['feature_std'] = numeric_df.std(axis=1).fillna(0)
            stats['feature_max'] = numeric_df.max(axis=1)
            stats['feature_min'] = numeric_df.min(axis=1)
            stats['feature_count_nonzero'] = (numeric_df > 0).sum(axis=1)
        
        return stats
    
    def apply_feature_selection(self, X: np.ndarray, y: np.ndarray, 
                               feature_names: List[str],
                               method: str = 'importance') -> Tuple[np.ndarray, List[str]]:
        """
        FÃ¼hrt Feature Selection basierend auf verschiedenen Methoden durch
        
        Args:
            X: Feature Matrix
            y: Target Vector
            feature_names: Namen der Features
            method: Methode ('variance', 'univariate', 'importance', 'combined')
            
        Returns:
            Tuple von (selected_X, selected_feature_names)
        """
        self.logger.info(f"ğŸ” FÃ¼hre Feature Selection durch: {method}")
        
        if method == 'variance':
            return self._variance_based_selection(X, feature_names)
        elif method == 'univariate':
            return self._univariate_selection(X, y, feature_names)
        elif method == 'importance':
            return self._importance_based_selection(X, y, feature_names)
        elif method == 'combined':
            return self._combined_selection(X, y, feature_names)
        else:
            self.logger.warning(f"âš ï¸ Unbekannte Selection-Methode: {method} - verwende 'importance'")
            return self._importance_based_selection(X, y, feature_names)
    
    def _variance_based_selection(self, X: np.ndarray, 
                                 feature_names: List[str]) -> Tuple[np.ndarray, List[str]]:
        """Feature Selection basierend auf Varianz"""
        selector = VarianceThreshold(threshold=MIN_FEATURE_VARIANCE)
        X_selected = selector.fit_transform(X)
        
        selected_mask = selector.get_support()
        selected_features = [feature_names[i] for i in range(len(feature_names)) if selected_mask[i]]
        
        removed_count = len(feature_names) - len(selected_features)
        self.logger.info(f"âœ… Variance Selection: {removed_count} Features entfernt (Varianz < {MIN_FEATURE_VARIANCE})")
        
        return X_selected, selected_features
    
    def _univariate_selection(self, X: np.ndarray, y: np.ndarray, 
                             feature_names: List[str], 
                             k: int = None) -> Tuple[np.ndarray, List[str]]:
        """Feature Selection basierend auf univariaten statistischen Tests"""
        if k is None:
            k = min(len(feature_names), VALIDATION_RULES['max_features'])
        
        selector = SelectKBest(score_func=f_classif, k=k)
        X_selected = selector.fit_transform(X, y)
        
        selected_mask = selector.get_support()
        selected_features = [feature_names[i] for i in range(len(feature_names)) if selected_mask[i]]
        
        # Speichere Scores fÃ¼r Analyse
        scores = selector.scores_
        for i, feature in enumerate(feature_names):
            if selected_mask[i]:
                self.feature_importance[feature] = float(scores[i])
        
        self.logger.info(f"âœ… Univariate Selection: Top {len(selected_features)} Features ausgewÃ¤hlt")
        
        return X_selected, selected_features
    
    def _importance_based_selection(self, X: np.ndarray, y: np.ndarray, 
                                   feature_names: List[str]) -> Tuple[np.ndarray, List[str]]:
        """Feature Selection basierend auf Random Forest Feature Importance"""
        # Quick Random Forest fÃ¼r Feature Importance
        rf = RandomForestClassifier(
            n_estimators=100,
            random_state=CV_RANDOM_STATE,
            n_jobs=-1
        )
        rf.fit(X, y)
        
        # Feature Importance
        importances = rf.feature_importances_
        
        # Sortiere Features nach Importance
        importance_indices = np.argsort(importances)[::-1]
        
        # WÃ¤hle Features Ã¼ber Threshold
        selected_indices = []
        for idx in importance_indices:
            if importances[idx] >= MIN_FEATURE_IMPORTANCE:
                selected_indices.append(idx)
        
        # Mindestens Top 20 Features, maximal max_features
        min_features = min(20, len(feature_names))
        max_features = min(VALIDATION_RULES['max_features'], len(feature_names))
        
        if len(selected_indices) < min_features:
            selected_indices = importance_indices[:min_features].tolist()
        elif len(selected_indices) > max_features:
            selected_indices = selected_indices[:max_features]
        
        # Erstelle Output
        X_selected = X[:, selected_indices]
        selected_features = [feature_names[i] for i in selected_indices]
        
        # Speichere Feature Importance
        for i, feature in enumerate(feature_names):
            self.feature_importance[feature] = float(importances[i])
        
        self.logger.info(f"âœ… Importance Selection: {len(selected_features)} Features (Threshold: {MIN_FEATURE_IMPORTANCE})")
        
        return X_selected, selected_features
    
    def _combined_selection(self, X: np.ndarray, y: np.ndarray, 
                           feature_names: List[str]) -> Tuple[np.ndarray, List[str]]:
        """Kombinierte Feature Selection (Varianz + Importance)"""
        # Schritt 1: Variance-based Selection
        X_var, features_var = self._variance_based_selection(X, feature_names)
        
        # Schritt 2: Importance-based Selection auf reduzierten Features
        X_final, features_final = self._importance_based_selection(X_var, y, features_var)
        
        self.logger.info(f"âœ… Combined Selection: {len(feature_names)} â†’ {len(features_var)} â†’ {len(features_final)} Features")
        
        return X_final, features_final
    
    def apply_sampling_strategy(self, X: np.ndarray, y: np.ndarray, 
                               strategy: str = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        Wendet Sampling-Strategien fÃ¼r Class Balancing an
        
        Args:
            X: Feature Matrix
            y: Target Vector
            strategy: Sampling-Strategie (default: DEFAULT_SAMPLING_CONFIG)
            
        Returns:
            Tuple von (X_resampled, y_resampled)
        """
        if strategy is None:
            strategy = DEFAULT_SAMPLING_CONFIG['strategy']
        
        self.logger.info(f"âš–ï¸ Wende Sampling-Strategie an: {strategy}")
        
        # Original Class Distribution
        unique, counts = np.unique(y, return_counts=True)
        original_ratio = counts[1] / counts[0] if len(counts) > 1 else 1.0
        self.logger.info(f"   Original Class Ratio: {original_ratio:.3f} (Positive/Negative)")
        
        if strategy == SAMPLING_METHODS['NONE']:
            return X, y
        
        try:
            # WÃ¤hle Sampling-Methode
            if strategy == SAMPLING_METHODS['SMOTE']:
                sampler = SMOTE(random_state=DEFAULT_SAMPLING_CONFIG['random_state'])
            elif strategy == SAMPLING_METHODS['BORDERLINE_SMOTE']:
                sampler = BorderlineSMOTE(random_state=DEFAULT_SAMPLING_CONFIG['random_state'])
            elif strategy == SAMPLING_METHODS['ADASYN']:
                sampler = ADASYN(random_state=DEFAULT_SAMPLING_CONFIG['random_state'])
            elif strategy == SAMPLING_METHODS['RANDOM_UNDER']:
                sampler = RandomUnderSampler(random_state=DEFAULT_SAMPLING_CONFIG['random_state'])
            elif strategy == SAMPLING_METHODS['TOMEK_LINKS']:
                sampler = TomekLinks()
            elif strategy == SAMPLING_METHODS['SMOTEENN']:
                sampler = SMOTEENN(random_state=DEFAULT_SAMPLING_CONFIG['random_state'])
            elif strategy == SAMPLING_METHODS['SMOTETOMEK']:
                sampler = SMOTETomek(random_state=DEFAULT_SAMPLING_CONFIG['random_state'])
            else:
                self.logger.warning(f"âš ï¸ Unbekannte Sampling-Strategie: {strategy} - verwende BorderlineSMOTE")
                sampler = BorderlineSMOTE(random_state=DEFAULT_SAMPLING_CONFIG['random_state'])
            
            # FÃ¼hre Sampling durch
            X_resampled, y_resampled = sampler.fit_resample(X, y)
            
            # Neue Class Distribution
            unique_new, counts_new = np.unique(y_resampled, return_counts=True)
            new_ratio = counts_new[1] / counts_new[0] if len(counts_new) > 1 else 1.0
            
            self.logger.info(f"âœ… Sampling abgeschlossen: {len(X)} â†’ {len(X_resampled)} Samples")
            self.logger.info(f"   Neue Class Ratio: {new_ratio:.3f}")
            
            return X_resampled, y_resampled
            
        except Exception as e:
            self.logger.error(f"âŒ Fehler beim Sampling: {e}")
            self.logger.info("   Verwende Original-Daten ohne Sampling")
            return X, y
    
    def prepare_training_data(self, df: pd.DataFrame, 
                             prediction_timebase: str,
                             target_column: str = None) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """
        Bereitet Daten fÃ¼r Model Training vor
        
        Args:
            df: DataFrame mit Features
            prediction_timebase: Zeitpunkt fÃ¼r Prediction (YYYYMM)
            target_column: Name der Target-Spalte
            
        Returns:
            Tuple von (X, y, feature_names)
        """
        self.logger.info("ğŸ¯ Bereite Training-Daten vor...")
        
        # Identifiziere Target-Spalte
        if target_column is None:
            target_column = self._identify_target_column(df)
        
        if target_column not in df.columns:
            raise ValueError(f"Target-Spalte '{target_column}' nicht in DataFrame gefunden")
        
        # Filter nach Prediction Timebase
        timebase_col = self._get_timebase_column(df)
        if timebase_col in df.columns:
            # Verwende nur Daten vor prediction_timebase fÃ¼r Training (Datentyp-Konvertierung)
            prediction_timebase_int = int(prediction_timebase) if isinstance(prediction_timebase, str) else prediction_timebase
            training_mask = df[timebase_col] < prediction_timebase_int
            training_df = df[training_mask].copy()
            self.logger.info(f"   Zeitfilter angewendet: {len(df)} â†’ {len(training_df)} Records")
        else:
            training_df = df.copy()
            self.logger.warning("âš ï¸ Keine Timebase-Spalte gefunden - verwende alle Daten")
        
        # Separate Features und Target (EXCLUDE SAME-PERIOD BUSINESS FEATURES fÃ¼r Data Leakage Prevention)
        # Exclusion-Liste: Business Activity Features aus derselben Periode (verursachen Data Leakage)
        same_period_business_features = [
            'I_MAINTENANCE', 'I_UPGRADE', 'I_UPSELL', 'I_DOWNGRADE', 
            'I_DOWNSELL', 'I_UHD', 'I_CONSULTING', 'I_SEMINAR', 
            'I_SOCIALINSURANCENOTES'  
        ]
        
        feature_columns = [col for col in training_df.columns 
                          if col not in [target_column, self._get_customer_column(df), timebase_col] + same_period_business_features]
        
        self.logger.warning(f"âš ï¸ DATA LEAKAGE PREVENTION: {len(same_period_business_features)} same-period Business Features ausgeschlossen")
        self.logger.info(f"ğŸ“Š Verwende {len(feature_columns)} leakage-freie Features fÃ¼r Training")
        
        X = training_df[feature_columns].fillna(0).values
        y = training_df[target_column].fillna(0).values
        
        # Validierung
        self._validate_training_data(X, y, feature_columns)
        
        self.feature_names = feature_columns
        
        self.logger.info(f"âœ… Training-Daten vorbereitet: {X.shape[0]} Samples, {X.shape[1]} Features")
        
        return X, y, feature_columns
    
    def _validate_training_data(self, X: np.ndarray, y: np.ndarray, 
                               feature_names: List[str]) -> None:
        """Validiert Training-Daten auf Konsistenz"""
        
        # Basic Validations
        if len(X) != len(y):
            raise ValueError(f"Feature Matrix ({len(X)}) und Target Vector ({len(y)}) haben unterschiedliche LÃ¤ngen")
        
        if len(X) < VALIDATION_RULES['min_samples']:
            raise ValueError(f"Zu wenig Training-Samples: {len(X)} < {VALIDATION_RULES['min_samples']}")
        
        if X.shape[1] != len(feature_names):
            raise ValueError(f"Feature Matrix Breite ({X.shape[1]}) != Feature Names LÃ¤nge ({len(feature_names)})")
        
        # Class Distribution Validation
        unique_classes, class_counts = np.unique(y, return_counts=True)
        if len(unique_classes) < 2:
            raise ValueError(f"Nur eine Klasse im Target gefunden: {unique_classes}")
        
        positive_ratio = class_counts[1] / len(y) if len(class_counts) > 1 else 0
        if positive_ratio < VALIDATION_RULES['min_positive_class_ratio']:
            raise ValueError(f"Positive Class Ratio zu niedrig: {positive_ratio:.3f} < {VALIDATION_RULES['min_positive_class_ratio']}")
        
        if positive_ratio > VALIDATION_RULES['max_positive_class_ratio']:
            self.logger.warning(f"âš ï¸ Positive Class Ratio sehr hoch: {positive_ratio:.3f}")
        
        # Feature Validation
        if X.shape[1] < VALIDATION_RULES['min_features']:
            raise ValueError(f"Zu wenig Features: {X.shape[1]} < {VALIDATION_RULES['min_features']}")
        
        # NaN/Inf Check
        if np.any(np.isnan(X)) or np.any(np.isinf(X)):
            nan_count = np.sum(np.isnan(X))
            inf_count = np.sum(np.isinf(X))
            raise ValueError(f"NaN ({nan_count}) oder Infinity ({inf_count}) Werte in Feature Matrix gefunden")
        
        self.logger.info(f"âœ… Training-Daten Validation bestanden")
        self.logger.info(f"   Classes: {dict(zip(unique_classes, class_counts))}")
        self.logger.info(f"   Positive Ratio: {positive_ratio:.3f}")
    
    # ==========================================
    # HELPER METHODS
    # ==========================================
    
    def _identify_temporal_features(self, df: pd.DataFrame) -> List[str]:
        """Identifiziert temporale Features basierend auf Data Dictionary"""
        if self.data_dictionary and 'columns' in self.data_dictionary:
            temporal_features = []
            for col, info in self.data_dictionary['columns'].items():
                if col in df.columns and info.get('type') == 'DYNAMIC_FEATURE':
                    temporal_features.append(col)
            return temporal_features
        else:
            # Fallback: Numerische Spalten (auÃŸer IDs)
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            return [col for col in numeric_cols if not col.lower().endswith('_id')]
    
    def _identify_business_activity_features(self, df: pd.DataFrame) -> List[str]:
        """Identifiziert Business Activity Features"""
        business_keywords = ['CONSULTING', 'UPGRADE', 'UPSELL', 'MAINTENANCE', 'SEMINAR', 'UHD']
        business_features = []
        
        for col in df.columns:
            if any(keyword in col.upper() for keyword in business_keywords):
                if df[col].dtype in ['int64', 'float64']:
                    business_features.append(col)
        
        return business_features
    
    def _identify_digitalization_features(self, df: pd.DataFrame) -> List[str]:
        """Identifiziert Digitalization Features"""
        digital_keywords = ['DIGITAL', 'RATE', 'ONLINE']
        digital_features = []
        
        for col in df.columns:
            if any(keyword in col.upper() for keyword in digital_keywords):
                digital_features.append(col)
        
        return digital_features
    
    def _get_customer_column(self, df: pd.DataFrame) -> str:
        """Ermittelt Customer ID Spalte"""
        if CUSTOMER_ID_FIELD in df.columns:
            return CUSTOMER_ID_FIELD
        elif 'customer_id' in df.columns:
            return 'customer_id'
        elif 'Kunde' in df.columns:
            return 'Kunde'
        else:
            # Fallback: Erste Spalte die wie ID aussieht
            for col in df.columns:
                if 'id' in col.lower() or 'kunde' in col.lower():
                    return col
            return df.columns[0]  # Last resort
    
    def _get_timebase_column(self, df: pd.DataFrame) -> str:
        """Ermittelt Timebase Spalte"""
        timebase_candidates = ['I_TIMEBASE', 'timebase', 'time', 'date']
        for candidate in timebase_candidates:
            if candidate in df.columns:
                return candidate
        return 'I_TIMEBASE'  # Default
    
    def _identify_target_column(self, df: pd.DataFrame) -> str:
        """Identifiziert Target-Spalte"""
        target_candidates = ['I_Alive', 'target', 'churn', 'y']
        for candidate in target_candidates:
            if candidate in df.columns:
                return candidate
        
        # Fallback: Spalte mit 0/1 oder True/False Werten
        for col in df.columns:
            if df[col].dtype in ['int64', 'float64', 'bool']:
                unique_vals = df[col].dropna().unique()
                if len(unique_vals) == 2 and (
                    set(unique_vals).issubset({0, 1}) or 
                    set(unique_vals).issubset({True, False})
                ):
                    return col
        
        raise ValueError("Keine Target-Spalte identifiziert")
    
    def _classify_feature_category(self, feature_name: str) -> str:
        """Klassifiziert Feature in Business-Kategorie"""
        feature_upper = feature_name.upper()
        
        if any(keyword in feature_upper for keyword in ['CONSULTING', 'UPGRADE', 'MAINTENANCE']):
            return FEATURE_CATEGORIES['BUSINESS_ACTIVITY']
        elif any(keyword in feature_upper for keyword in ['DIGITAL', 'RATE']):
            return FEATURE_CATEGORIES['DIGITALIZATION']
        elif any(keyword in feature_upper for keyword in ['FINANCIAL', 'COST', 'REVENUE']):
            return FEATURE_CATEGORIES['FINANCIAL']
        elif any(keyword in feature_upper for keyword in ['AGE', 'LOCATION', 'SIZE']):
            return FEATURE_CATEGORIES['DEMOGRAPHIC']
        else:
            return FEATURE_CATEGORIES['BEHAVIORAL']
    
    def get_feature_summary(self) -> Dict[str, Any]:
        """Erstellt Zusammenfassung der erstellten Features"""
        summary = {
            'total_features': len(self.feature_names),
            'feature_types': {},
            'feature_categories': {},
            'feature_importance': self.feature_importance,
            'feature_metadata': self.feature_metadata
        }
        
        # Aggregiere nach Types und Categories
        for feature, metadata in self.feature_metadata.items():
            feature_type = metadata.get('type', 'unknown')
            feature_category = metadata.get('category', 'unknown')
            
            summary['feature_types'][feature_type] = summary['feature_types'].get(feature_type, 0) + 1
            summary['feature_categories'][feature_category] = summary['feature_categories'].get(feature_category, 0) + 1
        
        return summary


if __name__ == "__main__":
    # Test der ChurnFeatureEngine FunktionalitÃ¤t
    from bl.Churn.churn_data_loader import ChurnDataLoader
    
    print("ğŸ§ª Teste ChurnFeatureEngine...")
    
    # 1. Lade Test-Daten
    data_loader = ChurnDataLoader()
    csv_path = str(ProjectPaths.input_data_directory() / "churn_Data_cleaned.csv")
    df = data_loader.load_stage0_data(csv_path)
    
    if df is None:
        print("âŒ Konnte Test-Daten nicht laden")
        exit(1)
    
    # 2. Initialisiere Feature Engine
    data_dict = data_loader.load_data_dictionary()
    feature_engine = ChurnFeatureEngine(data_dictionary=data_dict)
    
    # 3. Teste Feature Engineering
    print(f"   Original Features: {len(df.columns)}")
    
    # Rolling Features
    df_with_rolling = feature_engine.create_rolling_features(df.head(1000))  # Test mit Subset
    print(f"   Nach Rolling Features: {len(df_with_rolling.columns)}")
    
    # Enhanced Features
    df_with_enhanced = feature_engine.create_enhanced_features(df_with_rolling)
    print(f"   Nach Enhanced Features: {len(df_with_enhanced.columns)}")
    
    # 4. Training Data Preparation
    try:
        X, y, feature_names = feature_engine.prepare_training_data(
            df_with_enhanced, 
            prediction_timebase="202401"
        )
        print(f"âœ… Training Data: {X.shape[0]} Samples, {X.shape[1]} Features")
        
        # Feature Selection Test
        X_selected, selected_features = feature_engine.apply_feature_selection(
            X, y, feature_names, method='importance'
        )
        print(f"âœ… Feature Selection: {len(feature_names)} â†’ {len(selected_features)} Features")
        
    except Exception as e:
        print(f"âŒ Fehler bei Training Preparation: {e}")
    
    # 5. Feature Summary
    summary = feature_engine.get_feature_summary()
    print(f"âœ… Feature Summary: {summary['total_features']} Features")
    print(f"   Types: {summary['feature_types']}")
    
    print("\nğŸ¯ ChurnFeatureEngine Test erfolgreich abgeschlossen!")
